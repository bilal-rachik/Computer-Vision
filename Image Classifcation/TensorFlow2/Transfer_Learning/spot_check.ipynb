{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spot_check.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNo5JuCTp7SW"
      },
      "source": [
        "# Extracteurs et classificateurs à contrôle ponctuel.\n",
        "\n",
        "Souvent, lorsque nous abordons un nouveau projet, nous sommes victimes du paradoxe du choix : nous ne savons pas par où ni comment commencer en raison de la présence de tant d'options parmi lesquelles choisir. Quel extracteur de caractéristiques est le meilleur ? Quel est le modèle le plus performant que nous puissions entraîner ? Comment devons-nous pré-traiter nos données ?\n",
        "\n",
        "Dans cette recette, nous allons implémenter un framework qui vérifiera automatiquement les extracteurs et classificateurs de caractéristiques. L'objectif n'est pas d'obtenir le meilleur modèle possible tout de suite, mais de restreindre nos options afin que nous puissions nous concentrer sur les plus prometteurs à un stade ultérieur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gNvOwHusv8W"
      },
      "source": [
        "Nous utiliserons un ensemble de données appelé 17 Category Flower Dataset, disponible ici : http://www.robots.ox.ac.uk/~vgg/data/flowers/17. Cependant, une version organisée, organisée en sous-dossiers par classe, peut être téléchargée ici : https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip. Décompressez-le à l'endroit de votre choix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2azPTYyb7nSl"
      },
      "source": [
        "#!cp /content/drive/MyDrive/probabilité/flowers17.zip /content/\n",
        "#! unzip flowers17.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQg43Daut-f9"
      },
      "source": [
        "Les étapes suivantes nous permettront de vérifier plusieurs combinaisons d'extracteurs de caractéristiques et d'algorithmes d'apprentissage automatique. Suivez ces étapes pour terminer cette recette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zJ_eDDwuKgi"
      },
      "source": [
        "**1.** Importez les packages nécessaires :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RswkM21VuUgc"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "from glob import glob\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.linear_model import *\n",
        "import sklearn.utils as skutils\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import *\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.preprocessing.image import *\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m4vxkZvvKlV"
      },
      "source": [
        "**2.** nous réutiliserons la classe FeatureExtractor() que nous avons dénie dans Implémentation d'un extracteur de caractéristiques à l'aide d'une recette réseau pré-entraînée, au début de ce chapitre. Consultez-le si vous voulez en savoir plus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRa5fW2AvFc3"
      },
      "source": [
        "class FeatureExtractor(object):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 input_size,\n",
        "                 label_encoder,\n",
        "                 num_instances,\n",
        "                 feature_size,\n",
        "                 output_path,\n",
        "                 features_key='features',\n",
        "                 buffer_size=1000):\n",
        "      \n",
        "        #Nous devons nous assurer que le chemin de sortie peut être écrit\n",
        "\n",
        "        if os.path.exists(output_path):\n",
        "            error_msg = (f'{output_path} already exists. '\n",
        "                         f'Please delete it and try again.')\n",
        "            raise FileExistsError(error_msg)\n",
        "        # Maintenant, stockons le paramètre d'entrée en tant objet\n",
        "        self.model = model\n",
        "        self.input_size = input_size\n",
        "        self.le = label_encoder\n",
        "        self.feature_size = feature_size\n",
        "        \n",
        "        # self.buffer contiendra un tampon d'instances et d'étiquettes, \n",
        "        #tandis que self.current_index pointera vers le prochain emplacement\n",
        "        # libre dans les ensembles de données de la base de données interne HDF5. Nous allons créer ceci maintenant :\n",
        "\n",
        "        self.db = h5py.File(output_path, 'w')\n",
        "        self.features = self.db.create_dataset(features_key,\n",
        "                                               (num_instances,\n",
        "                                                feature_size),\n",
        "                                               dtype='float')\n",
        "        \n",
        "        self.labels = self.db.create_dataset('labels',\n",
        "                                             (num_instances,),\n",
        "                                             dtype='int')\n",
        "\n",
        "        self.buffer_size = buffer_size\n",
        "        self.buffer = {'features': [], 'labels': []}\n",
        "        self.current_index = 0\n",
        "\n",
        "    # Définir une méthode qui extraira les caractéristiques et les étiquettes d'une liste de chemins d'images et les stockera dans la base de données HDF5\n",
        "    def extract_features(self,\n",
        "                         image_paths,\n",
        "                         labels,\n",
        "                         batch_size=64,\n",
        "                         shuffle=True):\n",
        "        if shuffle:\n",
        "            image_paths, labels = skutils.shuffle(image_paths,\n",
        "                                                  labels)\n",
        "            \n",
        "        encoded_labels = self.le.fit_transform(labels)\n",
        "        self._store_class_labels(self.le.classes_)\n",
        "\n",
        "        # Après avoir mélangé les chemins d'images et leurs étiquettes,\n",
        "        # ainsi que l'encodage et le stockage de ces derniers, nous itérerons sur des lots d'images, en les passant à travers le réseau pré-entraîné. \n",
        "        #Une fois cela fait, nous enregistrerons les fonctionnalités résultantes dans la base de données HDF5 (les méthodes d'aide que nous avons utilisées ici seront définies sous peu)\n",
        "\n",
        "        for i in tqdm(range(0, len(image_paths), batch_size)):\n",
        "\n",
        "            batch_paths = image_paths[i: i + batch_size]\n",
        "            batch_labels = encoded_labels[i:i + batch_size]\n",
        "            batch_images = []\n",
        "\n",
        "            for image_path in batch_paths:\n",
        "                image = load_img(image_path,\n",
        "                                 target_size=self.input_size)\n",
        "                image = img_to_array(image)\n",
        "                image = np.expand_dims(image, axis=0)\n",
        "                image = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "                batch_images.append(image)\n",
        "\n",
        "            batch_images = np.vstack(batch_images)\n",
        "            feats = self.model.predict(batch_images,\n",
        "                                       batch_size=batch_size)\n",
        "\n",
        "            new_shape = (feats.shape[0], self.feature_size)\n",
        "            feats = feats.reshape(new_shape)\n",
        "            self._add(feats, batch_labels)\n",
        "\n",
        "        self._close()\n",
        "    # Définir une méthode privée qui ajoutera des entités et des étiquettes aux jeux de données correspondants\n",
        "    def _add(self, rows, labels):\n",
        "        self.buffer['features'].extend(rows)\n",
        "        self.buffer['labels'].extend(labels)\n",
        "\n",
        "        if len(self.buffer['features']) >= self.buffer_size:\n",
        "            self._flush()\n",
        "    # Définir une méthode privée qui videra le buffer sur le disque\n",
        "    def _flush(self):\n",
        "        next_index = (self.current_index +\n",
        "                      len(self.buffer['features']))\n",
        "        buffer_slice = slice(self.current_index, next_index)\n",
        "        self.features[buffer_slice] = self.buffer['features']\n",
        "        self.labels[buffer_slice] = self.buffer['labels']\n",
        "        self.current_index = next_index\n",
        "        self.buffer = {'features': [], 'labels': []}\n",
        "    # Définir une méthode privée qui stockera les étiquettes de classe dans la base de données HDF5\n",
        "    def _store_class_labels(self, class_labels):\n",
        "\n",
        "        print(\"class_labels=====>\", class_labels)\n",
        "        data_type = h5py.special_dtype(vlen=str)\n",
        "        label_ds = self.db.create_dataset('label_names',\n",
        "                                          (len(class_labels),),\n",
        "                                          dtype=data_type)\n",
        "        \n",
        "        label_ds[:] = class_labels\n",
        "    # Définir une méthode privée qui fermera le jeu de données HDF5\n",
        "    def _close(self):\n",
        "        if len(self.buffer['features']) > 0:\n",
        "            self._flush()\n",
        "\n",
        "        self.db.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM2BFXEpvVXO"
      },
      "source": [
        "**3.** Dénissez la taille d'entrée de tous les extracteurs de caractéristiques :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOw8HvCYvYhs"
      },
      "source": [
        "INPUT_SIZE = (224, 224, 3)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZcC93Rovb0F"
      },
      "source": [
        "**4.** Définissez une fonction qui obtiendra une liste de tuples de réseaux pré-entraînés, ainsi que la dimensionnalité des vecteurs qu'ils génèrent : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJsN8jtVvp49"
      },
      "source": [
        "def get_pretrained_networks():\n",
        "    return [\n",
        "        (VGG16(input_shape=INPUT_SIZE,\n",
        "               weights='imagenet',\n",
        "               include_top=False),\n",
        "         7 * 7 * 512),\n",
        "        (VGG19(input_shape=INPUT_SIZE,\n",
        "               weights='imagenet',\n",
        "               include_top=False),\n",
        "         7 * 7 * 512),\n",
        "        (Xception(input_shape=INPUT_SIZE,\n",
        "                  weights='imagenet',\n",
        "                  include_top=False),\n",
        "         7 * 7 * 2048),\n",
        "        (ResNet152V2(input_shape=INPUT_SIZE,\n",
        "                     weights='imagenet',\n",
        "                     include_top=False),\n",
        "         7 * 7 * 2048),\n",
        "        (InceptionResNetV2(input_shape=INPUT_SIZE,\n",
        "                           weights='imagenet',\n",
        "                           include_top=False),\n",
        "         5 * 5 * 1536)\n",
        "    ]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l47bhlV7v1Oe"
      },
      "source": [
        "**5.**Définissez une fonction qui renvoie un dict de modèles d'apprentissage automatique à vérifier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfmfFGN2v-4F"
      },
      "source": [
        "def get_classifiers():\n",
        "    models = {}\n",
        "    models['LogisticRegression'] = LogisticRegression()\n",
        "    models['SGDClf'] = SGDClassifier()\n",
        "    models['PAClf'] = PassiveAggressiveClassifier()\n",
        "    models['DecisionTreeClf'] = DecisionTreeClassifier()\n",
        "    models['ExtraTreeClf'] = ExtraTreeClassifier()\n",
        "\n",
        "    n_trees = 100\n",
        "    models[f'AdaBoostClf-{n_trees}'] = \\\n",
        "        AdaBoostClassifier(n_estimators=n_trees)\n",
        "    models[f'BaggingClf-{n_trees}'] = \\\n",
        "        BaggingClassifier(n_estimators=n_trees)\n",
        "    models[f'RandomForestClf-{n_trees}'] = \\\n",
        "        RandomForestClassifier(n_estimators=n_trees)\n",
        "    models[f'ExtraTreesClf-{n_trees}'] = \\\n",
        "        ExtraTreesClassifier(n_estimators=n_trees)\n",
        "    models[f'GradientBoostingClf-{n_trees}'] = \\\n",
        "        GradientBoostingClassifier(n_estimators=n_trees)\n",
        "\n",
        "    number_of_neighbors = range(3, 25)\n",
        "    for n in number_of_neighbors:\n",
        "        models[f'KNeighborsClf-{n}'] = \\\n",
        "            KNeighborsClassifier(n_neighbors=n)\n",
        "\n",
        "    reg = [1e-3, 1e-2, 1, 10]\n",
        "    for r in reg:\n",
        "        models[f'LinearSVC-{r}'] = LinearSVC(C=r)\n",
        "        models[f'RidgeClf-{r}'] = RidgeClassifier(alpha=r)\n",
        "\n",
        "    print(f'Defined {len(models)} models.')\n",
        "    return models"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxJdr3uAwYyc"
      },
      "source": [
        "**6.** Dénissez le chemin d'accès au jeu de données, ainsi qu'une liste de tous les chemins d'images :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL3frJqpwgnf"
      },
      "source": [
        "dataset_path = (pathlib.Path('/content')/ 'flowers17')\n",
        "files_pattern = (dataset_path / 'images' / '*' / '*.jpg')\n",
        "images_path = [*glob(str(files_pattern))]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sStIQ27_32sj"
      },
      "source": [
        "**7.** Charger les étiquettes en mémoire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGfpPoDW34Gx",
        "outputId": "38d72a76-0025-47e9-fbf4-16e6b50e8119"
      },
      "source": [
        "labels = []\n",
        "for index in tqdm(range(len(images_path))):\n",
        "    image_path = images_path[index]\n",
        "    image = load_img(image_path)\n",
        "\n",
        "    label = image_path.split(os.path.sep)[-2]\n",
        "    labels.append(label)\n",
        "\n",
        "    image.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1360/1360 [00:00<00:00, 9184.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHtme_tr4G1R"
      },
      "source": [
        "**8.** Déﬁnissez certaines variables afin de garder une trace du processus de contrôle ponctuel. final_report contiendra la précision de chaque classificateur, entraîné sur les caractéristiques produites par différents réseaux pré-entraînés. best_model, best_accuracy et best_features contiendront respectivement le nom du meilleur modèle, sa précision et le nom du réseau pré-entraîné qui a produit les caractéristiques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMQvCI24TFS"
      },
      "source": [
        "final_report = {}\n",
        "best_model = None\n",
        "best_accuracy = -1\n",
        "best_features = None"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxZ8ATni4ZfC"
      },
      "source": [
        "**9** Itérer sur chaque réseau pré-entraîné, en l'utilisant pour extraire des caractéristiques des images de l'ensemble de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV6PxWNC4rZq",
        "outputId": "8586a2fb-1bff-4235-917c-1a5b2a5883e4"
      },
      "source": [
        "for model, feature_size in get_pretrained_networks():\n",
        "    output_path = dataset_path / f'{model.name}_features.hdf5'\n",
        "    output_path = str(output_path)\n",
        "    fe = FeatureExtractor(model=model,\n",
        "                          input_size=INPUT_SIZE,\n",
        "                          label_encoder=LabelEncoder(),\n",
        "                          num_instances=len(images_path),\n",
        "                          feature_size=feature_size,\n",
        "                          output_path=output_path)\n",
        "  \n",
        "    fe.extract_features(image_paths=images_path,\n",
        "                        labels=labels)\n",
        "    #Prendre 80% des données pour former, et 20% pour tester\n",
        "    db = h5py.File(output_path, 'r')\n",
        "\n",
        "    TRAIN_PROPORTION = 0.8\n",
        "    SPLIT_INDEX = int(len(labels) * TRAIN_PROPORTION)\n",
        "\n",
        "    X_train, y_train = (db['features'][:SPLIT_INDEX],\n",
        "                        db['labels'][:SPLIT_INDEX])\n",
        "    X_test, y_test = (db['features'][SPLIT_INDEX:],\n",
        "                      db['labels'][SPLIT_INDEX:])\n",
        "\n",
        "    classifiers_report = {\n",
        "        'extractor': model.name\n",
        "    }\n",
        "\n",
        "    print(f'Spot-checking with features from {model.name}')\n",
        "\n",
        "    #À l'aide des fonctionnalités extraites dans l'itération actuelle,\n",
        "    #passez en revue tous les modèles d'apprentissage automatique, entraînez-les sur l'ensemble d'entraînement et évaluez-les sur l'ensemble de test\n",
        "    for clf_name, clf in get_classifiers().items():\n",
        "        try:\n",
        "            clf.fit(X_train, y_train)\n",
        "        except Exception as e:\n",
        "            print(f'\\t{clf_name}: {e}')\n",
        "            continue\n",
        "\n",
        "        predictions = clf.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "        print(f'\\t{clf_name}: {accuracy}')\n",
        "        classifiers_report[clf_name] = accuracy\n",
        "        #Vérifiez si nous avons un nouveau meilleur modèle. Si tel est le cas, mettez à jour les variables appropriées :\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = clf_name\n",
        "            best_features = model.name\n",
        "    #Stockez les résultats de cette itération dans final_report et libérez les ressources du chier HDF5\n",
        "    final_report[output_path] = classifiers_report\n",
        "    db.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 0s 0us/step\n",
            "83697664/83683744 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234553344/234545216 [==============================] - 2s 0us/step\n",
            "234561536/234545216 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n",
            "219070464/219055592 [==============================] - 2s 0us/step\n",
            "class_labels=====> ['bluebell' 'buttercup' 'coltsfoot' 'cowslip' 'crocus' 'daffodil' 'daisy'\n",
            " 'dandelion' 'fritillary' 'iris' 'lilyvalley' 'pansy' 'snowdrop'\n",
            " 'sunflower' 'tigerlily' 'tulip' 'windflower']\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [01:01<00:00,  2.78s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spot-checking with features from vgg16\n",
            "Defined 40 models.\n",
            "\tLogisticRegression: 0.8933823529411765\n",
            "\tSGDClf: 0.8970588235294118\n",
            "\tPAClf: 0.9007352941176471\n",
            "\tDecisionTreeClf: 0.4411764705882353\n",
            "\tExtraTreeClf: 0.41544117647058826\n",
            "\tAdaBoostClf-100: 0.12867647058823528\n",
            "\tBaggingClf-100: 0.7977941176470589\n",
            "\tRandomForestClf-100: 0.8014705882352942\n",
            "\tExtraTreesClf-100: 0.8125\n",
            "\tGradientBoostingClf-100: 0.6727941176470589\n",
            "\tKNeighborsClf-3: 0.49264705882352944\n",
            "\tKNeighborsClf-4: 0.4852941176470588\n",
            "\tKNeighborsClf-5: 0.5110294117647058\n",
            "\tKNeighborsClf-6: 0.5110294117647058\n",
            "\tKNeighborsClf-7: 0.5073529411764706\n",
            "\tKNeighborsClf-8: 0.5073529411764706\n",
            "\tKNeighborsClf-9: 0.4852941176470588\n",
            "\tKNeighborsClf-10: 0.5036764705882353\n",
            "\tKNeighborsClf-11: 0.5073529411764706\n",
            "\tKNeighborsClf-12: 0.5110294117647058\n",
            "\tKNeighborsClf-13: 0.5\n",
            "\tKNeighborsClf-14: 0.5073529411764706\n",
            "\tKNeighborsClf-15: 0.49264705882352944\n",
            "\tKNeighborsClf-16: 0.4852941176470588\n",
            "\tKNeighborsClf-17: 0.48161764705882354\n",
            "\tKNeighborsClf-18: 0.4852941176470588\n",
            "\tKNeighborsClf-19: 0.4889705882352941\n",
            "\tKNeighborsClf-20: 0.4889705882352941\n",
            "\tKNeighborsClf-21: 0.4889705882352941\n",
            "\tKNeighborsClf-22: 0.4963235294117647\n",
            "\tKNeighborsClf-23: 0.4889705882352941\n",
            "\tKNeighborsClf-24: 0.4852941176470588\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-0.001: 0.9044117647058824\n",
            "\tRidgeClf-0.001: 0.8602941176470589\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-0.01: 0.9044117647058824\n",
            "\tRidgeClf-0.01: 0.8602941176470589\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-1: 0.9044117647058824\n",
            "\tRidgeClf-1: 0.8602941176470589\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-10: 0.9044117647058824\n",
            "\tRidgeClf-10: 0.8639705882352942\n",
            "class_labels=====> ['bluebell' 'buttercup' 'coltsfoot' 'cowslip' 'crocus' 'daffodil' 'daisy'\n",
            " 'dandelion' 'fritillary' 'iris' 'lilyvalley' 'pansy' 'snowdrop'\n",
            " 'sunflower' 'tigerlily' 'tulip' 'windflower']\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [00:16<00:00,  1.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spot-checking with features from vgg19\n",
            "Defined 40 models.\n",
            "\tLogisticRegression: 0.9448529411764706\n",
            "\tSGDClf: 0.9080882352941176\n",
            "\tPAClf: 0.9485294117647058\n",
            "\tDecisionTreeClf: 0.49264705882352944\n",
            "\tExtraTreeClf: 0.3897058823529412\n",
            "\tAdaBoostClf-100: 0.09558823529411764\n",
            "\tBaggingClf-100: 0.7941176470588235\n",
            "\tRandomForestClf-100: 0.8345588235294118\n",
            "\tExtraTreesClf-100: 0.8382352941176471\n",
            "\tGradientBoostingClf-100: 0.7279411764705882\n",
            "\tKNeighborsClf-3: 0.5514705882352942\n",
            "\tKNeighborsClf-4: 0.5698529411764706\n",
            "\tKNeighborsClf-5: 0.5661764705882353\n",
            "\tKNeighborsClf-6: 0.5551470588235294\n",
            "\tKNeighborsClf-7: 0.5477941176470589\n",
            "\tKNeighborsClf-8: 0.5625\n",
            "\tKNeighborsClf-9: 0.5441176470588235\n",
            "\tKNeighborsClf-10: 0.5551470588235294\n",
            "\tKNeighborsClf-11: 0.5477941176470589\n",
            "\tKNeighborsClf-12: 0.5404411764705882\n",
            "\tKNeighborsClf-13: 0.5588235294117647\n",
            "\tKNeighborsClf-14: 0.5588235294117647\n",
            "\tKNeighborsClf-15: 0.5551470588235294\n",
            "\tKNeighborsClf-16: 0.5514705882352942\n",
            "\tKNeighborsClf-17: 0.5477941176470589\n",
            "\tKNeighborsClf-18: 0.5477941176470589\n",
            "\tKNeighborsClf-19: 0.5330882352941176\n",
            "\tKNeighborsClf-20: 0.5294117647058824\n",
            "\tKNeighborsClf-21: 0.5294117647058824\n",
            "\tKNeighborsClf-22: 0.5294117647058824\n",
            "\tKNeighborsClf-23: 0.5367647058823529\n",
            "\tKNeighborsClf-24: 0.5367647058823529\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-0.001: 0.9375\n",
            "\tRidgeClf-0.001: 0.9044117647058824\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-0.01: 0.9375\n",
            "\tRidgeClf-0.01: 0.9044117647058824\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-1: 0.9375\n",
            "\tRidgeClf-1: 0.9080882352941176\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLinearSVC-10: 0.9375\n",
            "\tRidgeClf-10: 0.9080882352941176\n",
            "class_labels=====> ['bluebell' 'buttercup' 'coltsfoot' 'cowslip' 'crocus' 'daffodil' 'daisy'\n",
            " 'dandelion' 'fritillary' 'iris' 'lilyvalley' 'pansy' 'snowdrop'\n",
            " 'sunflower' 'tigerlily' 'tulip' 'windflower']\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [00:21<00:00,  1.00it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spot-checking with features from xception\n",
            "Defined 40 models.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tLogisticRegression: 0.5551470588235294\n",
            "\tSGDClf: 0.5477941176470589\n",
            "\tPAClf: 0.5551470588235294\n",
            "\tDecisionTreeClf: 0.23529411764705882\n",
            "\tExtraTreeClf: 0.18382352941176472\n",
            "\tAdaBoostClf-100: 0.09558823529411764\n",
            "\tBaggingClf-100: 0.41544117647058826\n",
            "\tRandomForestClf-100: 0.40808823529411764\n",
            "\tExtraTreesClf-100: 0.41911764705882354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUDW4CgS7-rc"
      },
      "source": [
        "**10.**Mettez à jour final_report avec les informations du meilleur modèle. Enfin, écrivez-le sur le disque"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqB-DwluBjig"
      },
      "source": [
        "final_report['best_model'] = best_model\n",
        "final_report['best_accuracy'] = best_accuracy\n",
        "final_report['best_features'] = best_features\n",
        "with open('final_report.json', 'w') as f:\n",
        "    json.dump(final_report, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQu5xPIfB87P"
      },
      "source": [
        "En examinant le fichier final_report.json, nous pouvons voir que le meilleur modèle est un PAClf (PassiveAggressiveClassifier), qui a atteint une précision de 0,934 (93,4%) sur l'ensemble de test et a été formé sur les fonctionnalités que nous avons extraites d'un réseau VGG19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN_-fQ-zCfdR"
      },
      "source": [
        "Dans cette recette, nous avons développé un cadre qui nous a permis de vérifier automatiquement 40 algorithmes d'apprentissage automatique différents en utilisant les fonctionnalités produites par cinq réseaux pré-formés différents, résultant en 200 expériences. En tirant parti des résultats de cette approche, nous avons constaté que la meilleure combinaison de modèles pour ce problème particulier était un PassiveAggressiveClassifier formé sur des vecteurs produits par un réseau VGG19.\n",
        "\n",
        "Notez que nous ne nous sommes pas concentrés sur l'obtention de performances maximales, mais plutôt sur la prise d'une décision éclairée, basée sur des preuves tangibles, sur l'endroit où passer notre temps et nos ressources si nous devions optimiser un classificateur sur cet ensemble de données. Maintenant, nous savons que le réglage précis d'un classificateur passif agressif sera très probablement payant. Combien de temps nous aurait-il fallu pour arriver à cette conclusion ? Des heures ou peut-être des jours.\n",
        "\n",
        "Le pouvoir de laisser l'ordinateur faire le gros du travail est que nous n'avons pas à deviner et, en même temps, sommes libres de consacrer notre temps à d'autres tâches. C'est super, n'est-ce pas ?\n"
      ]
    }
  ]
}
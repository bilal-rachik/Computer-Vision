{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "incremental_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPCSoS1847J"
      },
      "source": [
        "# Utiliser l'apprentissage incrémental pour former un classificateur\n",
        "\n",
        "L'un des problèmes des bibliothèques d'apprentissage automatique traditionnelles, telles que scikit-learn, est qu'elles offrent rarement la possibilité d'entraîner des modèles sur de gros volumes de données, ce qui, par coïncidence, est le meilleur type de données pour les réseaux de neurones profonds. À quoi bon avoir de grandes quantités de données si nous ne pouvons pas les utiliser ?\n",
        "\n",
        "Heureusement, il existe un moyen de contourner cette limitation, et cela s'appelle l'apprentissage incrémental. Dans cette recette, nous utiliserons une puissante bibliothèque, creme, pour entraîner un classificateur sur un ensemble de données trop volumineux pour être stocké en mémoire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ei_Cwnv9cE-"
      },
      "source": [
        "nous utiliserons creme, une bibliothèque expérimentale spécialement conçue pour entraîner des modèles d'apprentissage automatique sur d'énormes ensembles de données trop volumineux pour être stockés en mémoire. Pour installer la crème, exécutez la commande suivante :."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRYdAud89lRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d657f0d7-8ced-4112-9c17-dbd4c53651aa"
      },
      "source": [
        "!pip install creme==0.5.1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting creme==0.5.1\n",
            "  Downloading creme-0.5.1-cp37-cp37m-manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from creme==0.5.1) (1.4.1)\n",
            "Collecting mmh3==2.5.1\n",
            "  Downloading mmh3-2.5.1.tar.gz (9.8 kB)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from creme==0.5.1) (1.19.5)\n",
            "Building wheels for collected packages: mmh3\n",
            "  Building wheel for mmh3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmh3: filename=mmh3-2.5.1-cp37-cp37m-linux_x86_64.whl size=39689 sha256=8a6de954546bcaa8a9a0afa71505cde4f9c283f87df7310f244e8877c61caa98\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/45/25/90e097a519143b2dca74cd93a056894a965f27908103e01799\n",
            "Successfully built mmh3\n",
            "Installing collected packages: mmh3, creme\n",
            "Successfully installed creme-0.5.1 mmh3-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqIJUkny9oy7"
      },
      "source": [
        "Nous utiliserons l'ensemble de données features.hdf5 que nous avons généré dans Implémentation d'un extracteur de caractéristiques à l'aide d'une recette de réseau pré-entraînée dans ce chapitre, qui contient des informations codées sur les images pivotées à partir de l'ensemble de données Stanford Cars. Nous supposons que l'ensemble de données se trouve à l'emplacement suivant : //content/sample_data/features.hdf5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6UWllU89q-K",
        "outputId": "b45a4c7a-44e3-459c-e34e-179e66b60102"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z076_AYo92YG",
        "outputId": "9c7b9a22-9257-4142-89b4-774c3dff665c"
      },
      "source": [
        "!kaggle datasets download -d jessicali9530/stanford-cars-dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading stanford-cars-dataset.zip to /content\n",
            "100% 1.82G/1.82G [00:21<00:00, 82.6MB/s]\n",
            "100% 1.82G/1.82G [00:21<00:00, 90.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sFUNdbhCT5K"
      },
      "source": [
        "#!unzip stanford-cars-dataset.zip"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQfeo9z1CyDc"
      },
      "source": [
        "Nous stockerons les caractéristiques extraites au format HDF5, un protocole hiérarchique binaire conçu pour stocker de très grands ensembles de données numériques sur disque, tout en conservant la facilité d'accès et de calcul au niveau des lignes. Vous pouvez en savoir plus sur HDF5 ici : https://portal.hdfgroup.org/display/HDF5/HDF5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYwNx_yRDHG6"
      },
      "source": [
        "Suivez ces étapes pour terminer cette recette :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVfIxRadDL6T"
      },
      "source": [
        "**1.** Importez tous les packages nécessaires :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuClnag4DXjz"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "import h5py\n",
        "from creme import stream\n",
        "from creme.linear_model import LogisticRegression\n",
        "from creme.metrics import Accuracy\n",
        "from creme.multiclass import OneVsRestClassifier\n",
        "from creme.preprocessing import StandardScaler"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR3ytGmUSStn"
      },
      "source": [
        "**2.** Déﬁnissez une fonction qui enregistrera un ensemble de données sous forme de chier CSV :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzZy5fEYSUqP"
      },
      "source": [
        "def write_dataset(output_path, feats, labels, batch_size):\n",
        "    feature_size = feats.shape[1]\n",
        "    csv_columns = ['class'] + [f'feature_{i}'\n",
        "                               for i in range(feature_size)]\n",
        "    #Nous aurons une colonne pour la classe de chaque caractéristique et autant de colonnes \n",
        "    #\"d'éléments dans chaque vecteur de caractéristique. Ensuite, écrivons le contenu du fichier CSV par lots, en commençant par l'en-tête\n",
        "    dataset_size = labels.shape[0]\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(f'{\",\".join(csv_columns)}\\n')\n",
        "        #Extraire le lot dans cette itération\n",
        "        for batch_number, index in \\\n",
        "                enumerate(range(0, dataset_size, batch_size)):\n",
        "            print(f'Processing batch {batch_number + 1} of '\n",
        "                  f'{int(dataset_size / float(batch_size))}')\n",
        "\n",
        "            batch_feats = feats[index: index + batch_size]\n",
        "            batch_labels = labels[index: index + batch_size]\n",
        "            # Maintenant, écrivez toutes les lignes du lot\n",
        "            for label, vector in \\\n",
        "                    zip(batch_labels, batch_feats):\n",
        "                vector = ','.join([str(v) for v in vector])\n",
        "                f.write(f'{label},{vector}\\n')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scVHO2NETC_3"
      },
      "source": [
        "**3.**Charger le jeu de données au format HDF5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnhvtZ2UTEv2"
      },
      "source": [
        "dataset_path = str(pathlib.Path('/content/car_ims_rotated') / 'features.hdf5')\n",
        "db = h5py.File(dataset_path, 'r')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aapYweAQTRi3"
      },
      "source": [
        "**4.** Définir l'index de division pour séparer les données en morceaux d'entraînement (80 %) et de test (20 %)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k98T7NHOTXcg"
      },
      "source": [
        "TRAIN_PROPORTION = 0.8\n",
        "SPLIT_INDEX = int(db['labels'].shape[0] * TRAIN_PROPORTION)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3F29zwvTgyO"
      },
      "source": [
        "**5.**Écrire les sous-ensembles d'entraînement et de test sur le disque en tant que fichiers CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKGujY6kTrcH",
        "outputId": "90938e8a-36c4-4097-b01b-817b434f673c"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "write_dataset('train.csv',\n",
        "              db['features'][:SPLIT_INDEX],\n",
        "              db['labels'][:SPLIT_INDEX],\n",
        "              BATCH_SIZE)\n",
        "write_dataset('test.csv',\n",
        "              db['features'][SPLIT_INDEX:],\n",
        "              db['labels'][SPLIT_INDEX:],\n",
        "              BATCH_SIZE)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1 of 25\n",
            "Processing batch 2 of 25\n",
            "Processing batch 3 of 25\n",
            "Processing batch 4 of 25\n",
            "Processing batch 5 of 25\n",
            "Processing batch 6 of 25\n",
            "Processing batch 7 of 25\n",
            "Processing batch 8 of 25\n",
            "Processing batch 9 of 25\n",
            "Processing batch 10 of 25\n",
            "Processing batch 11 of 25\n",
            "Processing batch 12 of 25\n",
            "Processing batch 13 of 25\n",
            "Processing batch 14 of 25\n",
            "Processing batch 15 of 25\n",
            "Processing batch 16 of 25\n",
            "Processing batch 17 of 25\n",
            "Processing batch 18 of 25\n",
            "Processing batch 19 of 25\n",
            "Processing batch 20 of 25\n",
            "Processing batch 21 of 25\n",
            "Processing batch 22 of 25\n",
            "Processing batch 23 of 25\n",
            "Processing batch 24 of 25\n",
            "Processing batch 25 of 25\n",
            "Processing batch 26 of 25\n",
            "Processing batch 1 of 6\n",
            "Processing batch 2 of 6\n",
            "Processing batch 3 of 6\n",
            "Processing batch 4 of 6\n",
            "Processing batch 5 of 6\n",
            "Processing batch 6 of 6\n",
            "Processing batch 7 of 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOKOXpA0Tyow"
      },
      "source": [
        "**6.**creme nous oblige à spécifier le type de chaque colonne du fichier CSV en tant que dict. exemple, le bloc suivant spécifie que la classe doit être codée en tant qu'entier, tandis que les colonnes restantes, correspondant aux fonctionnalités, doivent être de type flottant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbFa3_IKT7uJ"
      },
      "source": [
        "FEATURE_SIZE = db['features'].shape[1]\n",
        "types = {f'feature_{i}': float for i in range(FEATURE_SIZE)}\n",
        "types['class'] = int"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vio-9OEQUCiY"
      },
      "source": [
        "**7.** Dans le code suivant, nous définissons un pipeline de crème, où chaque entrée sera standardisée avant d'être transmise au classificateur. Parce qu'il s'agit d'un problème multi-classes, nous devons envelopper LogisticRegression avec OneVsRestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYgTpiPIUG0Q"
      },
      "source": [
        "model = StandardScaler()\n",
        "model |= OneVsRestClassifier(LogisticRegression())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nwAOwKPUPhK"
      },
      "source": [
        "**8.** Définir la précision comme métrique cible et créer un itérateur sur l'ensemble de données train.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gso3phNUQ48"
      },
      "source": [
        "metric = Accuracy()\n",
        "dataset = stream.iter_csv('train.csv',\n",
        "                          target_name='class',\n",
        "                          converters=types)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klnA4EmnUVna"
      },
      "source": [
        "**9.** Former le classificateur, un exemple à la fois. Imprimez la précision de fonctionnement tous les 100 exemples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO3_SYUmUbci",
        "outputId": "aabc365f-fe21-4d75-ff49-3719594778f4"
      },
      "source": [
        "print('Training started...')\n",
        "for i, (X, y) in enumerate(dataset):\n",
        "    predictions = model.predict_one(X)\n",
        "    model = model.fit_one(X, y)\n",
        "    metric = metric.update(y, predictions)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f'Update {i} - {metric}')\n",
        "\n",
        "print(f'Final - {metric}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "Update 0 - Accuracy: 0.00%\n",
            "Update 100 - Accuracy: 82.18%\n",
            "Update 200 - Accuracy: 87.06%\n",
            "Update 300 - Accuracy: 90.03%\n",
            "Update 400 - Accuracy: 91.52%\n",
            "Update 500 - Accuracy: 92.61%\n",
            "Update 600 - Accuracy: 93.18%\n",
            "Update 700 - Accuracy: 94.01%\n",
            "Update 800 - Accuracy: 94.51%\n",
            "Update 900 - Accuracy: 94.89%\n",
            "Update 1000 - Accuracy: 94.91%\n",
            "Update 1100 - Accuracy: 95.10%\n",
            "Update 1200 - Accuracy: 95.34%\n",
            "Update 1300 - Accuracy: 95.54%\n",
            "Update 1400 - Accuracy: 95.57%\n",
            "Update 1500 - Accuracy: 95.87%\n",
            "Update 1600 - Accuracy: 96.06%\n",
            "Update 1700 - Accuracy: 96.18%\n",
            "Update 1800 - Accuracy: 96.28%\n",
            "Update 1900 - Accuracy: 96.11%\n",
            "Update 2000 - Accuracy: 96.30%\n",
            "Update 2100 - Accuracy: 96.43%\n",
            "Update 2200 - Accuracy: 96.55%\n",
            "Update 2300 - Accuracy: 96.52%\n",
            "Update 2400 - Accuracy: 96.67%\n",
            "Update 2500 - Accuracy: 96.68%\n",
            "Update 2600 - Accuracy: 96.77%\n",
            "Update 2700 - Accuracy: 96.74%\n",
            "Update 2800 - Accuracy: 96.79%\n",
            "Update 2900 - Accuracy: 96.76%\n",
            "Update 3000 - Accuracy: 96.87%\n",
            "Update 3100 - Accuracy: 96.90%\n",
            "Update 3200 - Accuracy: 96.97%\n",
            "Update 3300 - Accuracy: 97.06%\n",
            "Update 3400 - Accuracy: 97.15%\n",
            "Update 3500 - Accuracy: 97.17%\n",
            "Update 3600 - Accuracy: 97.25%\n",
            "Update 3700 - Accuracy: 97.27%\n",
            "Update 3800 - Accuracy: 97.26%\n",
            "Update 3900 - Accuracy: 97.26%\n",
            "Update 4000 - Accuracy: 97.33%\n",
            "Update 4100 - Accuracy: 97.34%\n",
            "Update 4200 - Accuracy: 97.36%\n",
            "Update 4300 - Accuracy: 97.42%\n",
            "Update 4400 - Accuracy: 97.48%\n",
            "Update 4500 - Accuracy: 97.49%\n",
            "Update 4600 - Accuracy: 97.50%\n",
            "Update 4700 - Accuracy: 97.49%\n",
            "Update 4800 - Accuracy: 97.54%\n",
            "Update 4900 - Accuracy: 97.53%\n",
            "Update 5000 - Accuracy: 97.54%\n",
            "Update 5100 - Accuracy: 97.57%\n",
            "Update 5200 - Accuracy: 97.62%\n",
            "Update 5300 - Accuracy: 97.66%\n",
            "Update 5400 - Accuracy: 97.65%\n",
            "Update 5500 - Accuracy: 97.65%\n",
            "Update 5600 - Accuracy: 97.66%\n",
            "Update 5700 - Accuracy: 97.70%\n",
            "Update 5800 - Accuracy: 97.74%\n",
            "Update 5900 - Accuracy: 97.78%\n",
            "Update 6000 - Accuracy: 97.82%\n",
            "Update 6100 - Accuracy: 97.84%\n",
            "Update 6200 - Accuracy: 97.86%\n",
            "Update 6300 - Accuracy: 97.89%\n",
            "Update 6400 - Accuracy: 97.88%\n",
            "Update 6500 - Accuracy: 97.89%\n",
            "Final - Accuracy: 97.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6emG6gzUgf6"
      },
      "source": [
        "**10.**Créez un itérateur sur le fichier test.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV3wq5q8UmUP"
      },
      "source": [
        "metric = Accuracy()\n",
        "test_dataset = stream.iter_csv('test.csv',\n",
        "                               target_name='class',\n",
        "                               converters=types)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9peiFxoVUwlF"
      },
      "source": [
        "**11.** Évaluez le modèle sur l'ensemble de test une fois de plus, un échantillon à la fois"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPn-iy_vU0UZ",
        "outputId": "cad42e37-0b5a-4be0-a171-d913413ea900"
      },
      "source": [
        "print('Testing model...')\n",
        "for i, (X, y) in enumerate(test_dataset):\n",
        "    predictions = model.predict_one(X)\n",
        "    metric = metric.update(y, predictions)\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(f'(TEST) Update {i} - {metric}')\n",
        "\n",
        "print(f'(TEST) Final - {metric}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model...\n",
            "(TEST) Update 0 - Accuracy: 100.00%\n",
            "(TEST) Update 1000 - Accuracy: 98.20%\n",
            "(TEST) Final - Accuracy: 98.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_whIumkU5cQ"
      },
      "source": [
        "Après quelques minutes, nous devrions avoir un modèle avec une précision d'environ 99% sur l'ensemble de test. Nous verrons cela plus en détail dans la section suivante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itBscN2obIb5"
      },
      "source": [
        "Souvent, même si nous disposons d'énormes quantités de données, nous ne pouvons pas toutes les utiliser en raison de limitations matérielles ou logicielles (dans la recette Formation d'un classificateur simple sur les fonctionnalités extraites, nous avons dû utiliser 50%, car nous ne pouvions pas tout garder en mémoire). Cependant, avec l'apprentissage incrémentiel (également connu sous le nom d'apprentissage en ligne), nous pouvons former des modèles d'apprentissage automatique traditionnels par lots, de la même manière que nous pouvons le faire avec les réseaux de neurones.\n",
        "\n",
        "Dans cette recette, afin de saisir la totalité du vecteur de caractéristiques de notre jeu de données Stanford Cars, nous avons dû écrire à la fois les jeux d'entraînement et de test dans des fichiers CSV. Ensuite, nous avons formé LogisticRegression et l'avons enveloppé dans OneVsRestClassifier, qui a appris à détecter les degrés de rotation dans les vecteurs de caractéristiques des images. Enfin, nous avons atteint une précision très satisfaisante de 99% sur l'ensemble de test"
      ]
    }
  ]
}
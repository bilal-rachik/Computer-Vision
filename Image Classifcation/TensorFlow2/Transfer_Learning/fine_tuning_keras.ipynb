{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuning_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieiQ2IyWDjMc"
      },
      "source": [
        "# Fine-tuning  d'un réseau à l'aide de l'API Keras\n",
        "\n",
        "L'un des plus grands avantages de l'apprentissage par transfert est peut-être sa capacité à saisir le vent arrière produit par les connaissances encodées dans des réseaux pré-formés. En échangeant simplement les couches les moins profondes dans l'un de ces réseaux, nous pouvons obtenir des performances remarquables sur de nouveaux ensembles de données non liés, même si nos données sont petites. Pourquoi? Parce que les informations contenues dans les couches inférieures sont pratiquement universelles : elles codent des formes et des formes de base qui s'appliquent à presque tous les problèmes de vision par ordinateur.\n",
        "\n",
        "Dans cette recette, nous allons affiner un réseau VGG16 pré-entraîné sur un petit ensemble de données, en obtenant un score de haute précision par ailleurs improbable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAeFqd_hEYgU"
      },
      "source": [
        "Nous utiliserons un ensemble de données appelé 17 Category Flower Dataset, disponible ici : http://www.robots.ox.ac.uk/~vgg/data/flowers/17. Une version organisée en sous-dossiers par classe est disponible ici : https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip. Téléchargez-le et décompressez-le à l'emplacement de votre choix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeSTRz_jfGLU"
      },
      "source": [
        "#!cp /content/drive/MyDrive/probabilité/flowers17.zip /content/\n",
        "#!unzip flowers17.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv87PQhuEwNE"
      },
      "source": [
        "Le fine tuning est facile ! Suivez ces étapes pour terminer cette recette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKgg7Knhd4s8"
      },
      "source": [
        "**1.** Importez les dépendances nécessaires :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogJRVq-od6eb"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.preprocessing.image import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUK3oHjeeB0s"
      },
      "source": [
        "**2.** Définissez la graine aléatoire :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_jGeYGKeDkx"
      },
      "source": [
        "SEED = 999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4yl5rpUeOlO"
      },
      "source": [
        "**3.** Défnir une fonction qui construira un nouveau réseau à partir d'un modèle pré-entraîné, où les couches supérieures entièrement connectées seront toutes nouvelles et adaptées au problème à résoudre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ziYDHgeXR8"
      },
      "source": [
        "def build_network(base_model, classes):\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(units=256)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "\n",
        "    x = Dense(units=classes)(x)\n",
        "    output = Softmax()(x)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uVB424fefDO"
      },
      "source": [
        "**4.** Définir une fonction qui chargera les images et les étiquettes dans l'ensemble de données en tant que tableaux NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuhwRzNfem51"
      },
      "source": [
        "def load_images_and_labels(image_paths,\n",
        "                           target_size=(256, 256)):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image = load_img(image_path, target_size=target_size)\n",
        "        image = img_to_array(image)\n",
        "\n",
        "        label = image_path.split(os.path.sep)[-2]\n",
        "\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aidRMNuZeoAr"
      },
      "source": [
        "**5.** Chargez les chemins des images et extrayez-en l'ensemble des classes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh6m597GfQ1u"
      },
      "source": [
        "dataset_path = pathlib.Path('/content/flowers17') \n",
        "files_pattern = (dataset_path / 'images' / '*' / '*.jpg')\n",
        "image_paths = [*glob(str(files_pattern))]\n",
        "CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th4Qs9Uffd9v"
      },
      "source": [
        "**6.** Chargez les images et normalisez-les, encodez à one-hot les étiquettes avec LabelBinarizer() et divisez les données en sous-ensembles pour l'entraînement (80%) et les tests (20%) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2vrjJeSfm_s"
      },
      "source": [
        "X, y = load_images_and_labels(image_paths)\n",
        "X = X.astype('float') / 255.0\n",
        "y = LabelBinarizer().fit_transform(y)\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n",
        "                                     test_size=0.2,\n",
        "                                     random_state=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o8-7I6Xf3h9"
      },
      "source": [
        "**7.** Instanciez un VGG16 pré-entraîné, sans les couches supérieures. Spécifiez une forme d'entrée de 256x256x3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmPLLqMcf9gW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc39cefd-65ce-4f5b-da66-cae4b9fdde57"
      },
      "source": [
        "base_model = VGG16(weights='imagenet',\n",
        "                   include_top=False,\n",
        "                   input_tensor=Input(shape=(256, 256, 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYov6FAggOwl"
      },
      "source": [
        "#Gelez toutes les couches du modèle de base. Nous le faisons parce que nous ne voulons pas les recycler, mais utiliser leurs connaissances existantes\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwE947g-god0"
      },
      "source": [
        "**8.** Construisez le réseau complet avec un nouvel ensemble de couches au-dessus en utilisant build_network() (défini à l'étape 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X7_bUSUgyDc"
      },
      "source": [
        "model = build_network(base_model, len(CLASSES))\n",
        "model = Model(base_model.input, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gkEo9tsg7y0"
      },
      "source": [
        "**9.** Déﬁnissez la taille du lot et un ensemble d'augmentations à appliquer via ImageDataGenerator() :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAahT-oeg-Tk"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "augmenter = ImageDataGenerator(rotation_range=30,\n",
        "                               horizontal_flip=True,\n",
        "                               width_shift_range=0.1,\n",
        "                               height_shift_range=0.1,\n",
        "                               shear_range=0.2,\n",
        "                               zoom_range=0.2,\n",
        "                               fill_mode='nearest')\n",
        "train_generator = augmenter.flow(X_train, y_train, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGZP8gBjhEWT"
      },
      "source": [
        "**10.** Réchauffez le réseau. Cela signifie que nous n'entraînerons les nouvelles couches (les autres sont gelées) que pendant 20 époques, en utilisant RMSProp avec un taux d'apprentissage de 0,001. Enfin, nous évaluerons le réseau sur l'ensemble de test :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee7ccBKchwQ8",
        "outputId": "8974131b-f4cf-4cb2-c355-9a43852d92a1"
      },
      "source": [
        "WARMING_EPOCHS = 20\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-3),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=WARMING_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "17/17 [==============================] - 83s 2s/step - loss: 1.5857 - accuracy: 0.5340 - val_loss: 2.9972 - val_accuracy: 0.5221\n",
            "Epoch 2/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.7540 - accuracy: 0.7812 - val_loss: 2.3457 - val_accuracy: 0.5699\n",
            "Epoch 3/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.5446 - accuracy: 0.8631 - val_loss: 2.0590 - val_accuracy: 0.6029\n",
            "Epoch 4/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.4343 - accuracy: 0.8980 - val_loss: 1.3152 - val_accuracy: 0.6765\n",
            "Epoch 5/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.3570 - accuracy: 0.9154 - val_loss: 1.1266 - val_accuracy: 0.7096\n",
            "Epoch 6/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.3225 - accuracy: 0.9035 - val_loss: 1.1564 - val_accuracy: 0.7169\n",
            "Epoch 7/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.3018 - accuracy: 0.9256 - val_loss: 0.8495 - val_accuracy: 0.8051\n",
            "Epoch 8/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.2525 - accuracy: 0.9467 - val_loss: 0.7209 - val_accuracy: 0.8199\n",
            "Epoch 9/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.2315 - accuracy: 0.9421 - val_loss: 0.8544 - val_accuracy: 0.7941\n",
            "Epoch 10/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.2245 - accuracy: 0.9540 - val_loss: 0.5725 - val_accuracy: 0.8382\n",
            "Epoch 11/20\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.2256 - accuracy: 0.9458 - val_loss: 0.6431 - val_accuracy: 0.8015\n",
            "Epoch 12/20\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.2297 - accuracy: 0.9439 - val_loss: 0.5939 - val_accuracy: 0.8199\n",
            "Epoch 13/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1897 - accuracy: 0.9586 - val_loss: 0.6203 - val_accuracy: 0.8162\n",
            "Epoch 14/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1656 - accuracy: 0.9660 - val_loss: 0.5394 - val_accuracy: 0.8493\n",
            "Epoch 15/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1463 - accuracy: 0.9724 - val_loss: 0.7100 - val_accuracy: 0.8272\n",
            "Epoch 16/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1599 - accuracy: 0.9586 - val_loss: 0.8729 - val_accuracy: 0.7684\n",
            "Epoch 17/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1417 - accuracy: 0.9697 - val_loss: 0.7975 - val_accuracy: 0.8015\n",
            "Epoch 18/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1484 - accuracy: 0.9660 - val_loss: 0.9121 - val_accuracy: 0.7831\n",
            "Epoch 19/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1334 - accuracy: 0.9706 - val_loss: 0.7439 - val_accuracy: 0.7978\n",
            "Epoch 20/20\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1221 - accuracy: 0.9752 - val_loss: 0.7199 - val_accuracy: 0.7978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjb0yifciXSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f018e5e-3bd0-4e55-b26e-f3e382f2f9e9"
      },
      "source": [
        "result = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {result[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 3s 275ms/step - loss: 0.7199 - accuracy: 0.7978\n",
            "Test accuracy: 0.7977941036224365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JOJ3_b3iXxO"
      },
      "source": [
        "**11.** Maintenant que le réseau a été réchauffé, nous allons affiner les couches finales du modèle de base, en particulier à partir du 16 (rappelez-vous, l'indexation zéro), ainsi que les couches entièrement connectées, pour 50 époques, en utilisant SGD avec un taux d'apprentissage de 0,001 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYSccInAi1y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bdb4733-ac43-4b1a-a414-3787764cb420"
      },
      "source": [
        "EPOCHS = 50\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(lr=1e-3),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=EPOCHS)\n",
        "result = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {result[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "17/17 [==============================] - 23s 1s/step - loss: 0.1083 - accuracy: 0.9825 - val_loss: 0.5124 - val_accuracy: 0.8640\n",
            "Epoch 2/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1162 - accuracy: 0.9807 - val_loss: 0.4504 - val_accuracy: 0.8750\n",
            "Epoch 3/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1220 - accuracy: 0.9752 - val_loss: 0.4155 - val_accuracy: 0.8824\n",
            "Epoch 4/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0990 - accuracy: 0.9789 - val_loss: 0.3919 - val_accuracy: 0.8897\n",
            "Epoch 5/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1106 - accuracy: 0.9807 - val_loss: 0.3759 - val_accuracy: 0.8860\n",
            "Epoch 6/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0947 - accuracy: 0.9835 - val_loss: 0.3646 - val_accuracy: 0.8860\n",
            "Epoch 7/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1054 - accuracy: 0.9853 - val_loss: 0.3570 - val_accuracy: 0.8897\n",
            "Epoch 8/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1080 - accuracy: 0.9807 - val_loss: 0.3509 - val_accuracy: 0.8897\n",
            "Epoch 9/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1052 - accuracy: 0.9862 - val_loss: 0.3460 - val_accuracy: 0.8934\n",
            "Epoch 10/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1009 - accuracy: 0.9835 - val_loss: 0.3422 - val_accuracy: 0.8971\n",
            "Epoch 11/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1053 - accuracy: 0.9752 - val_loss: 0.3393 - val_accuracy: 0.8971\n",
            "Epoch 12/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1013 - accuracy: 0.9807 - val_loss: 0.3371 - val_accuracy: 0.9044\n",
            "Epoch 13/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1015 - accuracy: 0.9789 - val_loss: 0.3357 - val_accuracy: 0.9044\n",
            "Epoch 14/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1043 - accuracy: 0.9825 - val_loss: 0.3345 - val_accuracy: 0.9044\n",
            "Epoch 15/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1023 - accuracy: 0.9807 - val_loss: 0.3337 - val_accuracy: 0.9081\n",
            "Epoch 16/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0975 - accuracy: 0.9825 - val_loss: 0.3329 - val_accuracy: 0.9081\n",
            "Epoch 17/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0985 - accuracy: 0.9835 - val_loss: 0.3323 - val_accuracy: 0.9044\n",
            "Epoch 18/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1064 - accuracy: 0.9853 - val_loss: 0.3320 - val_accuracy: 0.9044\n",
            "Epoch 19/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1051 - accuracy: 0.9862 - val_loss: 0.3317 - val_accuracy: 0.9044\n",
            "Epoch 20/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0993 - accuracy: 0.9835 - val_loss: 0.3317 - val_accuracy: 0.9044\n",
            "Epoch 21/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1020 - accuracy: 0.9816 - val_loss: 0.3316 - val_accuracy: 0.9044\n",
            "Epoch 22/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0859 - accuracy: 0.9853 - val_loss: 0.3316 - val_accuracy: 0.9044\n",
            "Epoch 23/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1036 - accuracy: 0.9825 - val_loss: 0.3315 - val_accuracy: 0.9044\n",
            "Epoch 24/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1180 - accuracy: 0.9770 - val_loss: 0.3314 - val_accuracy: 0.9044\n",
            "Epoch 25/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0890 - accuracy: 0.9862 - val_loss: 0.3318 - val_accuracy: 0.9044\n",
            "Epoch 26/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1100 - accuracy: 0.9807 - val_loss: 0.3313 - val_accuracy: 0.9081\n",
            "Epoch 27/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1110 - accuracy: 0.9816 - val_loss: 0.3314 - val_accuracy: 0.9081\n",
            "Epoch 28/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1045 - accuracy: 0.9798 - val_loss: 0.3309 - val_accuracy: 0.9081\n",
            "Epoch 29/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1048 - accuracy: 0.9743 - val_loss: 0.3311 - val_accuracy: 0.9081\n",
            "Epoch 30/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1045 - accuracy: 0.9807 - val_loss: 0.3311 - val_accuracy: 0.9118\n",
            "Epoch 31/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0961 - accuracy: 0.9807 - val_loss: 0.3311 - val_accuracy: 0.9118\n",
            "Epoch 32/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1000 - accuracy: 0.9779 - val_loss: 0.3312 - val_accuracy: 0.9118\n",
            "Epoch 33/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1037 - accuracy: 0.9816 - val_loss: 0.3315 - val_accuracy: 0.9118\n",
            "Epoch 34/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1078 - accuracy: 0.9816 - val_loss: 0.3314 - val_accuracy: 0.9118\n",
            "Epoch 35/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1042 - accuracy: 0.9835 - val_loss: 0.3313 - val_accuracy: 0.9118\n",
            "Epoch 36/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0957 - accuracy: 0.9807 - val_loss: 0.3310 - val_accuracy: 0.9118\n",
            "Epoch 37/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0934 - accuracy: 0.9862 - val_loss: 0.3312 - val_accuracy: 0.9118\n",
            "Epoch 38/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0919 - accuracy: 0.9853 - val_loss: 0.3308 - val_accuracy: 0.9118\n",
            "Epoch 39/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0947 - accuracy: 0.9871 - val_loss: 0.3308 - val_accuracy: 0.9118\n",
            "Epoch 40/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1017 - accuracy: 0.9770 - val_loss: 0.3309 - val_accuracy: 0.9118\n",
            "Epoch 41/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1121 - accuracy: 0.9807 - val_loss: 0.3310 - val_accuracy: 0.9118\n",
            "Epoch 42/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0877 - accuracy: 0.9871 - val_loss: 0.3308 - val_accuracy: 0.9118\n",
            "Epoch 43/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0983 - accuracy: 0.9807 - val_loss: 0.3303 - val_accuracy: 0.9118\n",
            "Epoch 44/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1016 - accuracy: 0.9825 - val_loss: 0.3300 - val_accuracy: 0.9118\n",
            "Epoch 45/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.1031 - accuracy: 0.9816 - val_loss: 0.3297 - val_accuracy: 0.9118\n",
            "Epoch 46/50\n",
            "17/17 [==============================] - 22s 1s/step - loss: 0.0964 - accuracy: 0.9844 - val_loss: 0.3300 - val_accuracy: 0.9118\n",
            "Epoch 47/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0943 - accuracy: 0.9816 - val_loss: 0.3298 - val_accuracy: 0.9118\n",
            "Epoch 48/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0965 - accuracy: 0.9844 - val_loss: 0.3297 - val_accuracy: 0.9118\n",
            "Epoch 49/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.0890 - accuracy: 0.9825 - val_loss: 0.3297 - val_accuracy: 0.9118\n",
            "Epoch 50/50\n",
            "17/17 [==============================] - 21s 1s/step - loss: 0.1079 - accuracy: 0.9807 - val_loss: 0.3296 - val_accuracy: 0.9118\n",
            "9/9 [==============================] - 3s 274ms/step - loss: 0.3296 - accuracy: 0.9118\n",
            "Test accuracy: 0.9117646813392639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rvysrxKi-J3"
      },
      "source": [
        "Après le préchauffage, le réseau a atteint une précision de 79,77 % sur l'ensemble de test. Ensuite, lorsque nous l'avons affiné, après 50 époques, la précision est passée à 91,17% sur l'ensemble de test. Nous verrons comment tout cela fonctionne dans la section suivante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg63P0FCjfme"
      },
      "source": [
        "Nous avons réussi à exploiter les connaissances d'un VGG16 pré-entraîné sur l'énorme base de données ImageNet. En remplaçant les couches supérieures, entièrement connectées et chargées de la classification proprement dite (les autres faisant office d'extracteurs de caractéristiques), par notre propre ensemble de couches profondes adaptées à notre problème, nous avons réussi à obtenir un 91,7 % de précision sur l'ensemble de test.\n",
        "\n",
        "Ce résultat est une démonstration de la puissance de l'apprentissage par transfert, d'autant plus que nous n'avons que 81 images par classe dans l'ensemble de données (81x17 = 1 377 au total), une quantité insuffisante pour former un modèle d'apprentissage en profondeur performant à partir de zéro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_hKeyEIkUJv"
      },
      "source": [
        "Vous pouvez en savoir plus sur les modèles pré-entraînés Keras ici : https://www.tensorflow.org/api_docs/python/tf/keras/applications"
      ]
    }
  ]
}
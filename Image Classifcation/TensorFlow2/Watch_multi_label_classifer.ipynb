{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Watch_multi-label classifer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80qLzDhcoxq7"
      },
      "source": [
        "# Création d'un classificateur multi-label pour étiqueter les montres.\n",
        "\n",
        "Un réseau de neurones ne se limite pas à modéliser la distribution d'une seule variable. En fait, il peut facilement gérer les cas où chaque image est associée à plusieurs étiquettes. Dans cette recette, nous allons implémenter un CNN pour classer le genre et le style/l'usage des montres. Commençons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZWIRx67pKYy"
      },
      "source": [
        "nous utiliserons l'ensemble de données Fashion Product Images (Small) hébergé dans Kaggle, que, après vous être connecté, vous pouvez télécharger ici : https://www.kaggle.com/paramaggarwal/fashion-product-images-small ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-0hFMhpn5m"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8RnEEmKpqYd"
      },
      "source": [
        "Vous devrez télécharger votre fichier kaggle.json pour cette étape, qui peut être obtenu à partir de votre compte Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DimXqFtppsl1"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5um0VKPSp9Dw",
        "outputId": "34161384-b54c-4b53-c6c3-b716dddf9bfc"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaGN2oKcp5EC"
      },
      "source": [
        "#!!kaggle datasets download -d paramaggarwal/fashion-product-images-small"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8akelqjlrF3I"
      },
      "source": [
        "#!unzip fashion-product-images-small.zip"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn_L-FXZrfJx"
      },
      "source": [
        "Passons en revue les étapes pour terminer la recette:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1bLoB4WrlX4"
      },
      "source": [
        "**1.** Importez les packages nécessaires :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJEmysRMrqPs"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "from csv import DictReader\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import *"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIcXnODDry1d"
      },
      "source": [
        "** 2.** Dénir une fonction pour construire l'architecture du réseau. Tout d'abord, implémentez les blocs convolutifs :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wuXoYhRr7IP"
      },
      "source": [
        "def build_network(width, height, depth, classes):\n",
        "    input_layer = Input(shape=(width, height, depth))\n",
        "\n",
        "    x = Conv2D(filters=32,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(input_layer)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Conv2D(filters=32,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.25)(x)\n",
        "\n",
        "    x = Conv2D(filters=64,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Conv2D(filters=64,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.25)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=512)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Dropout(rate=0.25)(x)\n",
        "\n",
        "    x = Dense(units=classes)(x)\n",
        "    output = Activation('sigmoid')(x)\n",
        "\n",
        "    return Model(input_layer, output)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtraZPmVsLy8"
      },
      "source": [
        "**3.** Dénissez une fonction pour charger toutes les images et les étiquettes (genre et usage), à ​​partir d'une liste de chemins d'images et d'un dictionnaire de métadonnées associé à chacune d'entre elles :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xYOX8ZusgiU"
      },
      "source": [
        "def load_images_and_labels(image_paths, styles, target_size):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image = load_img(image_path, target_size=target_size)\n",
        "        image = img_to_array(image)\n",
        "        image_id = image_path.split(os.path.sep)[-1][:-4]\n",
        "\n",
        "        image_style = styles[image_id]\n",
        "        label = (image_style['gender'], image_style['usage'])\n",
        "\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaX8MFIntRYA"
      },
      "source": [
        "**4.** Définissez la graine aléatoire pour garantir la reproductibilité :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhSbx8qbtbES"
      },
      "source": [
        "SEED = 999\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrbr8vZqtdWO"
      },
      "source": [
        "**5.** Dénissez les chemins d'accès aux images et au fichier de métadonnées styles.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWfIoe9ntm5q"
      },
      "source": [
        "base_path = pathlib.Path('/content/myntradataset')\n",
        "styles_path = str(base_path / 'styles.csv')\n",
        "images_path_pattern = str(base_path / 'images/*.jpg')\n",
        "image_paths = glob.glob(images_path_pattern)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n503OsbvuKTK"
      },
      "source": [
        "**6.** Conservez uniquement les images des montres pour une utilisation décontractée, décontractée et formelle, adaptée aux hommes et aux femmes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9yw_QQtuPAu"
      },
      "source": [
        "with open(styles_path, 'r') as f:\n",
        "    dict_reader = DictReader(f)\n",
        "    STYLES = [*dict_reader]\n",
        "\n",
        "    article_type = 'Watches'\n",
        "    genders = {'Men', 'Women'}\n",
        "    usages = {'Casual', 'Smart Casual', 'Formal'}\n",
        "    STYLES = {style['id']: style\n",
        "              for style in STYLES\n",
        "              if (style['articleType'] == article_type and\n",
        "                  style['gender'] in genders and\n",
        "                  style['usage'] in usages)}\n",
        "\n",
        "image_paths = [*filter(lambda p: p.split(os.path.sep)[-1][:-4]\n",
        "                                 in STYLES.keys(),\n",
        "                       image_paths)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdnPBCpiu6cP"
      },
      "source": [
        "**7.** Chargez les images et les étiquettes, en redimensionnant les images dans une forme 64x64x3 :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-SGo3BkvCVK"
      },
      "source": [
        "X, y = load_images_and_labels(image_paths, STYLES, (64, 64))\n",
        "X = X.astype('float') / 255.0"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjvaK9--vYuS"
      },
      "source": [
        "\n",
        "**8.** Normalisez les images et encodez les étiquettes en multi-hot :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1qF49QCvc0s"
      },
      "source": [
        "#X = X.astype('float') / 255.0\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(y)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqNBGXW1v31N"
      },
      "source": [
        "**9.** Créez les fractionnements d'entraînement, de validation et de test :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XddAkj8mv-SM"
      },
      "source": [
        "(X_train, X_test,\n",
        " y_train, y_test) = train_test_split(X, y,\n",
        "                                     stratify=y,\n",
        "                                     test_size=0.2,\n",
        "                                     random_state=SEED)\n",
        "(X_train, X_valid,\n",
        " y_train, y_valid) = train_test_split(X_train, y_train,\n",
        "                                      stratify=y_train,\n",
        "                                      test_size=0.2,\n",
        "                                      random_state=SEED)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1oBer6fwImG"
      },
      "source": [
        "**10.** Construisez et compilez le réseau :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHS4X63BwN-z"
      },
      "source": [
        "model = build_network(width=64,\n",
        "                      height=64,\n",
        "                      depth=3,\n",
        "                      classes=len(mlb.classes_))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tntUaed6wm2j"
      },
      "source": [
        "**11.** Entraînez le modèle pour 20 époques, par lots de 64 images à la fois :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6T97wvqwqoS",
        "outputId": "c0b8b19d-e2d2-4075-bc3c-e4fc0649d775"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 33s 97ms/step - loss: 0.6418 - accuracy: 0.1679 - val_loss: 0.9724 - val_accuracy: 0.0027\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 1s 59ms/step - loss: 0.4181 - accuracy: 0.2893 - val_loss: 0.7068 - val_accuracy: 0.4058\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.2923 - accuracy: 0.4267 - val_loss: 0.6802 - val_accuracy: 0.6393\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 1s 59ms/step - loss: 0.2120 - accuracy: 0.5627 - val_loss: 0.4499 - val_accuracy: 0.9814\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.1642 - accuracy: 0.6549 - val_loss: 0.5956 - val_accuracy: 0.9814\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.1327 - accuracy: 0.6835 - val_loss: 1.8023 - val_accuracy: 0.9735\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.1085 - accuracy: 0.6808 - val_loss: 0.4444 - val_accuracy: 0.9814\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.1012 - accuracy: 0.6954 - val_loss: 1.2314 - val_accuracy: 0.9814\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0840 - accuracy: 0.7173 - val_loss: 1.5325 - val_accuracy: 0.9814\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0668 - accuracy: 0.7313 - val_loss: 1.2288 - val_accuracy: 0.9814\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.0666 - accuracy: 0.7067 - val_loss: 2.7792 - val_accuracy: 0.9735\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0520 - accuracy: 0.7047 - val_loss: 2.2209 - val_accuracy: 0.9788\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.0462 - accuracy: 0.7021 - val_loss: 6.0500 - val_accuracy: 0.4324\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0491 - accuracy: 0.7319 - val_loss: 1.8808 - val_accuracy: 0.9788\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0339 - accuracy: 0.6894 - val_loss: 2.3001 - val_accuracy: 0.9708\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.0350 - accuracy: 0.6589 - val_loss: 0.7622 - val_accuracy: 0.9814\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0313 - accuracy: 0.6729 - val_loss: 0.7851 - val_accuracy: 0.9814\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0323 - accuracy: 0.6914 - val_loss: 0.4211 - val_accuracy: 0.9337\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 1s 59ms/step - loss: 0.0294 - accuracy: 0.6569 - val_loss: 0.6002 - val_accuracy: 0.8594\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.0267 - accuracy: 0.6722 - val_loss: 0.2975 - val_accuracy: 0.9125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17ae508410>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLvEERgxFu0"
      },
      "source": [
        "**12.** Évaluez le modèle sur l'ensemble de test :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN8uQT0wxRoT",
        "outputId": "e86d780e-9cd5-4998-da51-20e6c684ba3d"
      },
      "source": [
        "result = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(f'Test accuracy: {result[1]}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 47ms/step - loss: 0.3343 - accuracy: 0.9130\n",
            "Test accuracy: 0.9129511713981628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NswZW64CxfDj"
      },
      "source": [
        "**13.** Utilisez le modèle pour faire des prédictions sur une image de test, affichant la probabilité de chaque étiquette :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVXB5A3Ixlr1",
        "outputId": "101e66b6-1f4e-49d7-bf94-b4ef21067e3b"
      },
      "source": [
        "test_image = np.expand_dims(X_test[0], axis=0)\n",
        "probabilities = model.predict(test_image)[0]\n",
        "for label, p in zip(mlb.classes_, probabilities):\n",
        "    print(f'{label}: {p * 100:.2f}%')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Casual: 99.92%\n",
            "Formal: 0.00%\n",
            "Men: 53.19%\n",
            "Smart Casual: 0.00%\n",
            "Women: 63.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HweBDVhvyHhi"
      },
      "source": [
        "**14.** Comparez les étiquettes de vérité avec la prédiction du réseau :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YliBGtdyONK",
        "outputId": "5269b9aa-d02e-4086-d0f3-0067afc9f8ca"
      },
      "source": [
        "ground_truth_labels = np.expand_dims(y_test[0], axis=0)\n",
        "ground_truth_labels = mlb.inverse_transform(ground_truth_labels)\n",
        "print(f'Ground truth labels: {ground_truth_labels}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth labels: [('Casual', 'Women')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H4HDvyhy1uA"
      },
      "source": [
        "Nous avons implémenté une version plus petite d'un réseau VGG, capable d'effectuer une classification multi-étiquettes et multi-classes, en modélisant des distributions indépendantes pour les métadonnées de genre et d'utilisation associées à chaque montre. En d'autres termes, nous avons modélisé deux problèmes de classification binaire en même temps : un pour le genre et un pour l'usage. C'est la raison pour laquelle nous avons activé les sorties du réseau avec Sigmoid, au lieu de Sofmax, et aussi pourquoi la fonction de perte utilisée est binary_crossentropy et non catégorique_crossentropy.\n",
        "\n",
        "Nous avons entraîné le réseau susmentionné sur 20 époques, sur des lots de 64 images à la fois, obtenant une précision respectable de 90 % sur l'ensemble de test. Enfin, nous avons fait une prédiction sur une image invisible du jeu de test et vérifié que les étiquettes produites avec une grande certitude par le réseau (100 % de certitude pour Casual et 99,16 % pour Women) correspondent aux catégories de vérité  Casual et Women ."
      ]
    }
  ]
}
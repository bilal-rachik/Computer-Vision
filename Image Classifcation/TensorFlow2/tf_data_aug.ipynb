{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_data_aug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP0m5l6WeN2h"
      },
      "source": [
        "# Utilisation de l'augmentation des données pour améliorer les performances avec les API tf.data et tf.image.\n",
        "\n",
        "L'augmentation de données est une technique puissante que nous pouvons appliquer pour incrémenter artificiellement la taille de notre ensemble de données, en créant des copies légèrement modifiées des images à notre disposition. Dans cette recette, nous tirerons parti des API tf.data et tf.image pour augmenter les performances d'un CNN formé sur le jeu de données difficile Caltech 101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzEDRf5vesNK"
      },
      "source": [
        "Dans cette recette, nous utiliserons le jeu de données Caltech 101, disponible ici : http://www.vision.caltech.edu/Image_Datasets/Caltech101/. Téléchargez et décompressez 101_ObjectCategories.tar.gz à votre emplacement préféré.\n",
        "\n",
        "https://drive.google.com/u/0/uc?export=download&confirm=k34b&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jUexHLNfWW4"
      },
      "source": [
        "#!wget -c http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2f1NHqlfkWr"
      },
      "source": [
        "#!tar xvf 101_ObjectCategories.tar.gz"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G9no3Qee6dj"
      },
      "source": [
        "**1.** Importez les modules requis :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly8Il3zsm04p"
      },
      "source": [
        "#!pip install git+https://github.com/tensorflow/docs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMgZC9safEjg"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UVMPtOfxXR-"
      },
      "source": [
        "**2.** Créez un alias pour le  tf.data.experimental.AUTOTUNE, que nous utiliserons plus tard :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fim9czHrxfOf"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q9VP47OxqC-"
      },
      "source": [
        "**3.** Dénissez une fonction pour créer une version plus petite de VGG. Commencez par créer la couche d'entrée et le premier bloc de deux convolutions de 32 filtres chacune :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFttLN66x0c9"
      },
      "source": [
        "def build_network(width, height, depth, classes):\n",
        "    input_layer = Input(shape=(width, height, depth))\n",
        "\n",
        "    x = Conv2D(filters=32,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(input_layer)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Conv2D(filters=32,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.25)(x)\n",
        "\n",
        "    x = Conv2D(filters=64,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Conv2D(filters=64,\n",
        "               kernel_size=(3, 3),\n",
        "               padding='same')(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.25)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=512)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = BatchNormalization(axis=-1)(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "\n",
        "    x = Dense(units=classes)(x)\n",
        "    output = Softmax()(x)\n",
        "\n",
        "    return Model(input_layer, output)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZRykZTmfLBJ"
      },
      "source": [
        "**4.** Dénissez une fonction pour charger toutes les images de l'ensemble de données, ainsi que leurs étiquettes, en fonction de leurs chemins de fichiers :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDjV_jB0nHy_"
      },
      "source": [
        "def load_image_and_label(image_path, target_size=(64, 64)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, np.float32)\n",
        "    image = tf.image.resize(image, target_size)\n",
        "\n",
        "    label = tf.strings.split(image_path, os.path.sep)[-2]\n",
        "    label = (label == CLASSES)  # One-hot encode.\n",
        "    label = tf.dtypes.cast(label, tf.float32)\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgNdn1jknhiQ"
      },
      "source": [
        "**5.** Déﬁnissez une fonction pour tracer et enregistrer la courbe d'entraînement d'un modèle :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFKSZiulnnOA"
      },
      "source": [
        "def plot_model_history(model_history, metric, plot_name):\n",
        "    plt.style.use('seaborn-darkgrid')\n",
        "    plotter = tfdocs.plots.HistoryPlotter()\n",
        "    plotter.plot({'Model': model_history}, metric=metric)\n",
        "\n",
        "    plt.title(f'{metric.upper()}')\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "    plt.savefig(f'{plot_name}.png')\n",
        "    plt.close()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii4w5cRLyvt6"
      },
      "source": [
        "**6.** Déﬁnissez une fonction pour augmenter une image en effectuant des transformations aléatoires dessus :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CrTlrynyz34"
      },
      "source": [
        "def augment(image, label):\n",
        "    image = tf.image.resize_with_crop_or_pad(image, 74, 74)\n",
        "    image = tf.image.random_crop(image, size=(64, 64, 3))\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, 0.2)\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9t_EI1Jy-FJ"
      },
      "source": [
        "**7.** Dénissez une fonction pour préparer un tf.data.Dataset d'images, basé sur un modèle de type glob qui fait référence au dossier dans lequel elles résident :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv5ZSr4azKxa"
      },
      "source": [
        "def prepare_dataset(data_pattern):\n",
        "    return (tf.data.Dataset\n",
        "            .from_tensor_slices(data_pattern)\n",
        "            .map(load_image_and_label,\n",
        "                 num_parallel_calls=AUTOTUNE))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5LWwjPhnxEy"
      },
      "source": [
        "**8.** Définissez la graine aléatoire :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlKVtrK-ng7y"
      },
      "source": [
        "SEED = 999\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfop2RtlpRYi"
      },
      "source": [
        "**9.** Chargez les chemins vers toutes les images de l'ensemble de données, à l'exception de celles de la classe BACKGROUND_Google :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ptgFk6upayq"
      },
      "source": [
        "base_path = (pathlib.Path('/content') / '101_ObjectCategories')\n",
        "images_pattern = str(base_path / '*' / '*.jpg')\n",
        "image_paths = [*glob(images_pattern)]\n",
        "image_paths = [p for p in image_paths if\n",
        "               p.split(os.path.sep)[-2] != 'BACKGROUND_Google']"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnVHRNn-ppVx"
      },
      "source": [
        "**10\n",
        ".** Calculez les catégories uniques dans l'ensemble de données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIlqJ5Vopvw5"
      },
      "source": [
        "CLASSES = np.unique([p.split(os.path.sep)[-2]\n",
        "                     for p in image_paths])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX1LBij3p3Sq"
      },
      "source": [
        "**11.** Divisez les chemins d'images en sous-ensembles d'entraînement et de test :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzlgFDI9qBB5"
      },
      "source": [
        "train_paths, test_paths = train_test_split(image_paths,\n",
        "                                           test_size=0.2,\n",
        "                                           random_state=SEED)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL3TGnQT0VF1"
      },
      "source": [
        "**12.** Préparez les ensembles de données d'entraînement et de test, sans augmentation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaA2WOQ0Ywg"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1024\n",
        "train_dataset = (prepare_dataset(train_paths)\n",
        "                 .batch(BATCH_SIZE)\n",
        "                 .shuffle(buffer_size=BUFFER_SIZE)\n",
        "                 .prefetch(buffer_size=BUFFER_SIZE))\n",
        "test_dataset = (prepare_dataset(test_paths)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .prefetch(buffer_size=BUFFER_SIZE))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZHLO5goqJsE"
      },
      "source": [
        "**13.** Instancier, compiler, former et évaluer le réseau :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5kgiTIxqJDo",
        "outputId": "a8f4df30-245c-4594-ef3c-94549a6cc23b"
      },
      "source": [
        "EPOCHS = 40\n",
        "model = build_network(64, 64, 3, len(CLASSES))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=test_dataset,\n",
        "                    epochs=EPOCHS)\n",
        "result = model.evaluate(test_dataset)\n",
        "print(f'Test accuracy: {result[1]}')\n",
        "plot_model_history(history, 'accuracy', 'normal')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "109/109 [==============================] - 16s 74ms/step - loss: 2.9201 - accuracy: 0.4106 - val_loss: 4.8998 - val_accuracy: 0.0847\n",
            "Epoch 2/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 1.6160 - accuracy: 0.6243 - val_loss: 4.8924 - val_accuracy: 0.0806\n",
            "Epoch 3/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.9280 - accuracy: 0.7774 - val_loss: 3.5355 - val_accuracy: 0.2851\n",
            "Epoch 4/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.4874 - accuracy: 0.8960 - val_loss: 1.9808 - val_accuracy: 0.5593\n",
            "Epoch 5/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.2633 - accuracy: 0.9519 - val_loss: 1.5918 - val_accuracy: 0.6382\n",
            "Epoch 6/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.1447 - accuracy: 0.9797 - val_loss: 1.7000 - val_accuracy: 0.6094\n",
            "Epoch 7/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0950 - accuracy: 0.9849 - val_loss: 1.6153 - val_accuracy: 0.6486\n",
            "Epoch 8/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0748 - accuracy: 0.9860 - val_loss: 2.4743 - val_accuracy: 0.6141\n",
            "Epoch 9/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0576 - accuracy: 0.9921 - val_loss: 1.5373 - val_accuracy: 0.6659\n",
            "Epoch 10/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0448 - accuracy: 0.9941 - val_loss: 1.6657 - val_accuracy: 0.6417\n",
            "Epoch 11/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0483 - accuracy: 0.9914 - val_loss: 2.4501 - val_accuracy: 0.6152\n",
            "Epoch 12/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0342 - accuracy: 0.9947 - val_loss: 1.7673 - val_accuracy: 0.6498\n",
            "Epoch 13/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0336 - accuracy: 0.9952 - val_loss: 1.8041 - val_accuracy: 0.6555\n",
            "Epoch 14/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0234 - accuracy: 0.9970 - val_loss: 1.7291 - val_accuracy: 0.6452\n",
            "Epoch 15/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0319 - accuracy: 0.9947 - val_loss: 1.6723 - val_accuracy: 0.6532\n",
            "Epoch 16/40\n",
            "109/109 [==============================] - 14s 73ms/step - loss: 0.0207 - accuracy: 0.9973 - val_loss: 2.0125 - val_accuracy: 0.6164\n",
            "Epoch 17/40\n",
            "109/109 [==============================] - 14s 74ms/step - loss: 0.0241 - accuracy: 0.9961 - val_loss: 1.8864 - val_accuracy: 0.6371\n",
            "Epoch 18/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0196 - accuracy: 0.9961 - val_loss: 1.7976 - val_accuracy: 0.6486\n",
            "Epoch 19/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0191 - accuracy: 0.9958 - val_loss: 1.8178 - val_accuracy: 0.6480\n",
            "Epoch 20/40\n",
            "109/109 [==============================] - 14s 73ms/step - loss: 0.0172 - accuracy: 0.9964 - val_loss: 1.7871 - val_accuracy: 0.6659\n",
            "Epoch 21/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0152 - accuracy: 0.9968 - val_loss: 2.8360 - val_accuracy: 0.6181\n",
            "Epoch 22/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0110 - accuracy: 0.9981 - val_loss: 1.9353 - val_accuracy: 0.6429\n",
            "Epoch 23/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.0156 - accuracy: 0.9973 - val_loss: 1.8656 - val_accuracy: 0.6567\n",
            "Epoch 24/40\n",
            "109/109 [==============================] - 14s 73ms/step - loss: 0.0114 - accuracy: 0.9974 - val_loss: 1.9226 - val_accuracy: 0.6521\n",
            "Epoch 25/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0131 - accuracy: 0.9967 - val_loss: 1.8095 - val_accuracy: 0.6590\n",
            "Epoch 26/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0122 - accuracy: 0.9977 - val_loss: 2.0407 - val_accuracy: 0.6158\n",
            "Epoch 27/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 1.8563 - val_accuracy: 0.6561\n",
            "Epoch 28/40\n",
            "109/109 [==============================] - 14s 73ms/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 1.9838 - val_accuracy: 0.6532\n",
            "Epoch 29/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0085 - accuracy: 0.9980 - val_loss: 1.8454 - val_accuracy: 0.6636\n",
            "Epoch 30/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 1.9231 - val_accuracy: 0.6480\n",
            "Epoch 31/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0087 - accuracy: 0.9984 - val_loss: 2.2518 - val_accuracy: 0.6371\n",
            "Epoch 32/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 1.8285 - val_accuracy: 0.6567\n",
            "Epoch 33/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0078 - accuracy: 0.9991 - val_loss: 2.2653 - val_accuracy: 0.6077\n",
            "Epoch 34/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.0077 - accuracy: 0.9984 - val_loss: 1.8704 - val_accuracy: 0.6578\n",
            "Epoch 35/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 3.1285 - val_accuracy: 0.5985\n",
            "Epoch 36/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 1.8413 - val_accuracy: 0.6619\n",
            "Epoch 37/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 1.9209 - val_accuracy: 0.6469\n",
            "Epoch 38/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 1.9939 - val_accuracy: 0.6434\n",
            "Epoch 39/40\n",
            "109/109 [==============================] - 14s 72ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 2.1139 - val_accuracy: 0.6365\n",
            "Epoch 40/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 1.9938 - val_accuracy: 0.6446\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 1.9938 - accuracy: 0.6446\n",
            "Test accuracy: 0.6445852518081665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sHTzgZo095N"
      },
      "source": [
        "**14.** Préparez les ensembles d'entraînement et de test, cette fois en appliquant l'augmentation de données à l'ensemble d'entraînement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dxON8O61F7O"
      },
      "source": [
        "train_dataset = (prepare_dataset(train_paths)\n",
        "                 .map(augment, num_parallel_calls=AUTOTUNE)\n",
        "                 .batch(BATCH_SIZE)\n",
        "                 .shuffle(buffer_size=BUFFER_SIZE)\n",
        "                 .prefetch(buffer_size=BUFFER_SIZE))\n",
        "test_dataset = (prepare_dataset(test_paths)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .prefetch(buffer_size=BUFFER_SIZE))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlCH1c0U1QJd"
      },
      "source": [
        "**15.** Instanciez, compilez, entraînez et évaluez le réseau sur les données augmentées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV4EQxy-1TdW",
        "outputId": "b0e2c5f3-6501-4aa1-8d67-c81d904af620"
      },
      "source": [
        "model = build_network(64, 64, 3, len(CLASSES))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=test_dataset,\n",
        "                    epochs=EPOCHS)\n",
        "result = model.evaluate(test_dataset)\n",
        "print(f'Test accuracy: {result[1]}')\n",
        "plot_model_history(history, 'accuracy', 'augmented')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "109/109 [==============================] - 16s 72ms/step - loss: 3.6235 - accuracy: 0.3037 - val_loss: 6.3118 - val_accuracy: 0.0121\n",
            "Epoch 2/40\n",
            "109/109 [==============================] - 14s 69ms/step - loss: 2.6895 - accuracy: 0.4216 - val_loss: 6.7092 - val_accuracy: 0.0190\n",
            "Epoch 3/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 2.2102 - accuracy: 0.4949 - val_loss: 4.4150 - val_accuracy: 0.1861\n",
            "Epoch 4/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 1.8942 - accuracy: 0.5558 - val_loss: 2.3251 - val_accuracy: 0.4643\n",
            "Epoch 5/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 1.7065 - accuracy: 0.5905 - val_loss: 1.7374 - val_accuracy: 0.5893\n",
            "Epoch 6/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 1.5330 - accuracy: 0.6240 - val_loss: 1.5593 - val_accuracy: 0.6267\n",
            "Epoch 7/40\n",
            "109/109 [==============================] - 14s 69ms/step - loss: 1.3972 - accuracy: 0.6512 - val_loss: 1.6511 - val_accuracy: 0.6285\n",
            "Epoch 8/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 1.2855 - accuracy: 0.6748 - val_loss: 1.4224 - val_accuracy: 0.6630\n",
            "Epoch 9/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 1.1992 - accuracy: 0.6928 - val_loss: 1.3897 - val_accuracy: 0.6573\n",
            "Epoch 10/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 1.1256 - accuracy: 0.7124 - val_loss: 1.4288 - val_accuracy: 0.6763\n",
            "Epoch 11/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 1.0696 - accuracy: 0.7260 - val_loss: 1.4849 - val_accuracy: 0.6809\n",
            "Epoch 12/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.9975 - accuracy: 0.7388 - val_loss: 1.3952 - val_accuracy: 0.6722\n",
            "Epoch 13/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.9202 - accuracy: 0.7621 - val_loss: 1.3797 - val_accuracy: 0.6820\n",
            "Epoch 14/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.8872 - accuracy: 0.7652 - val_loss: 1.3969 - val_accuracy: 0.6884\n",
            "Epoch 15/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.8361 - accuracy: 0.7809 - val_loss: 1.2878 - val_accuracy: 0.6976\n",
            "Epoch 16/40\n",
            "109/109 [==============================] - 15s 70ms/step - loss: 0.8005 - accuracy: 0.7869 - val_loss: 1.4456 - val_accuracy: 0.6763\n",
            "Epoch 17/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.7704 - accuracy: 0.7997 - val_loss: 1.2461 - val_accuracy: 0.7120\n",
            "Epoch 18/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.7250 - accuracy: 0.8085 - val_loss: 1.1719 - val_accuracy: 0.7126\n",
            "Epoch 19/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 0.7048 - accuracy: 0.8126 - val_loss: 1.2091 - val_accuracy: 0.7097\n",
            "Epoch 20/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.6714 - accuracy: 0.8195 - val_loss: 1.2433 - val_accuracy: 0.7016\n",
            "Epoch 21/40\n",
            "109/109 [==============================] - 14s 70ms/step - loss: 0.6399 - accuracy: 0.8274 - val_loss: 1.2627 - val_accuracy: 0.7074\n",
            "Epoch 22/40\n",
            "109/109 [==============================] - 14s 71ms/step - loss: 0.6109 - accuracy: 0.8382 - val_loss: 1.1746 - val_accuracy: 0.7247\n",
            "Epoch 23/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.5911 - accuracy: 0.8411 - val_loss: 1.1385 - val_accuracy: 0.7298\n",
            "Epoch 24/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.5478 - accuracy: 0.8529 - val_loss: 1.1802 - val_accuracy: 0.7247\n",
            "Epoch 25/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.5446 - accuracy: 0.8469 - val_loss: 1.1667 - val_accuracy: 0.7195\n",
            "Epoch 26/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.5215 - accuracy: 0.8566 - val_loss: 1.1983 - val_accuracy: 0.7114\n",
            "Epoch 27/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.5023 - accuracy: 0.8620 - val_loss: 1.1782 - val_accuracy: 0.7298\n",
            "Epoch 28/40\n",
            "109/109 [==============================] - 15s 73ms/step - loss: 0.4771 - accuracy: 0.8698 - val_loss: 1.2099 - val_accuracy: 0.7362\n",
            "Epoch 29/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.4799 - accuracy: 0.8695 - val_loss: 1.1496 - val_accuracy: 0.7275\n",
            "Epoch 30/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.4470 - accuracy: 0.8785 - val_loss: 1.3317 - val_accuracy: 0.7154\n",
            "Epoch 31/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.4301 - accuracy: 0.8790 - val_loss: 1.2246 - val_accuracy: 0.7206\n",
            "Epoch 32/40\n",
            "109/109 [==============================] - 15s 73ms/step - loss: 0.4225 - accuracy: 0.8837 - val_loss: 1.1225 - val_accuracy: 0.7391\n",
            "Epoch 33/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.4075 - accuracy: 0.8895 - val_loss: 1.1958 - val_accuracy: 0.7275\n",
            "Epoch 34/40\n",
            "109/109 [==============================] - 15s 73ms/step - loss: 0.3862 - accuracy: 0.8937 - val_loss: 1.1773 - val_accuracy: 0.7247\n",
            "Epoch 35/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.3902 - accuracy: 0.8921 - val_loss: 1.0946 - val_accuracy: 0.7471\n",
            "Epoch 36/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.3719 - accuracy: 0.8984 - val_loss: 1.1848 - val_accuracy: 0.7316\n",
            "Epoch 37/40\n",
            "109/109 [==============================] - 15s 73ms/step - loss: 0.3549 - accuracy: 0.9033 - val_loss: 1.1935 - val_accuracy: 0.7195\n",
            "Epoch 38/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.3563 - accuracy: 0.9010 - val_loss: 1.1718 - val_accuracy: 0.7350\n",
            "Epoch 39/40\n",
            "109/109 [==============================] - 15s 71ms/step - loss: 0.3337 - accuracy: 0.9121 - val_loss: 1.2909 - val_accuracy: 0.7108\n",
            "Epoch 40/40\n",
            "109/109 [==============================] - 15s 72ms/step - loss: 0.3218 - accuracy: 0.9130 - val_loss: 1.1681 - val_accuracy: 0.7356\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 1.1681 - accuracy: 0.7356\n",
            "Test accuracy: 0.7355991005897522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtXqk98p17eo"
      },
      "source": [
        "Nous venons d'implémenter une version allégée de la célèbre architecture VGG, entraînée sur le jeu de données Caltech 101. Pour mieux comprendre les avantages de l'augmentation de données, nous avons installé une première version sur les données d'origine, sans aucune modication, obtenant un niveau de précision de 64,45 % sur l'ensemble de test. Ce premier modèle montre des signes de dépassement, car l'écart qui sépare les courbes de précision d'apprentissage et de validation s'élargit au début du processus d'apprentissage.\n",
        "\n",
        "Ensuite, nous avons entraîné le même réseau sur un jeu de données augmenté, en utilisant la fonction augment() définie précédemment. Cela a grandement amélioré les performances du modèle, atteignant une précision respectable de 73,55% sur l'ensemble de test. De plus, l'écart entre les courbes de précision d'entraînement et de validation est sensiblement plus petit, ce qui suggère un effet de régularisation résultant de l'application de l'augmentation des données.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
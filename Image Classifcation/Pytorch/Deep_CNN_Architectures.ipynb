{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_CNN_Architectures.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qkWjGTvhG6N"
      },
      "source": [
        "# Deep CNN Architectures\n",
        "\n",
        "Dans cette parie, nous passerons d'abord brièvement en revue l'évolution des CNN (en termes d'architectures), puis nous étudierons en détail les diérentes architectures de CNN. Nous allons implémenter ces architectures CNN à l'aide de PyTorch et, ce faisant, nous visons à explorer de manière exhaustive les outils (modules et fonctions intégrées) que PyTorch doit offrir dans le cadre de la construction de CNN profonds. Construire une solide expertise CNN dans PyTorch nous permettra de résoudre un certain nombre de problèmes d'apprentissage en profondeur impliquant des CNN. Cela nous aidera également à créer des modèles ou des applications d'apprentissage en profondeur plus complexes dont les CNN font partie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRsUSfJOhlsg"
      },
      "source": [
        "## Pourquoi les CNN sont-ils si puissants?\n",
        "\n",
        "Les CNN sont parmi les modèles d'apprentissage automatique les plus puissants pour résoudre des problèmes difficiles tels que la classification d'images, la détection d'objets, la segmentation d'objets, le traitement vidéo, le traitement du langage naturel et la reconnaissance vocale. Leur succès est attribué à divers facteurs, tels que les suivants:\n",
        "\n",
        "* Partage des poids : cela rend les CNN efficaces en termes de paramètres, c'est-à-dire que différentes caractéristiques sont extraites à l'aide du même ensemble de poids ou de paramètres. Les caractéristiques sont les représentations de haut niveau des données d'entrée que le modèle génère avec ses paramètres.\n",
        "\n",
        "* Extraction automatique d'entités : plusieurs étapes d'extraction d'entités aident un CNN à apprendre automatiquement les représentations d'entités dans un jeu de données.\n",
        "\n",
        "* Apprentissage hiérarchique : la structure CNN multicouche aide les CNN à apprendre les fonctionnalités de bas, moyen et haut niveau.\n",
        "\n",
        "* La capacité à explorer les corrélations spatiales et temporelles dans les données, comme dans les tâches de traitement vidéo.\n",
        "\n",
        "\n",
        "Outre ces caractéristiques fondamentales préexistantes, les CNN ont progressé au fil des ans grâce à des améliorations dans les domaines suivants:\n",
        "\n",
        "* L'utilisation de meilleures fonctions d'activation et de perte, telles que l'utilisation de ReLU pour surmonter le problème de la disparition de gradient. C'est quoi le problème de la disparition de gradient? Eh bien, nous savons que la rétropropagation dans les réseaux de neurones fonctionne sur la base de la règle de la chaîne de différenciation.\n",
        "\n",
        "Selon la règle de la chaîne, le gradient de la fonction de perte par rapport aux paramètres de la couche d'entrée peut être écrit comme un produit des gradients à chaque couche. Si ces gradients sont tous inférieurs à 1 – et pire encore, tendent vers 0 – alors le produit de ces gradients sera une valeur extrêmement faible. Le problème de la disparitionde  gradient peut causer de sérieux problèmes dans le processus d'optimisation en empêchant les paramètres du réseau de changer leurs valeurs, ce qui équivaut à un apprentissage retardé.\n",
        "\n",
        "* Optimisation des paramètres, telle que l'utilisation d'un optimiseur basé sur Adaptive Momentum (Adam) au lieu d'une simple descente de gradient stochastique.\n",
        "\n",
        "* Régularisation : application des Dropouts et de la normalisation par lots en plus de la régularisation L2.\n",
        "\n",
        "\n",
        "Mais certains des moteurs de développement les plus importants des CNN au fil des ans ont été les diverses innovations architecturales :\n",
        "\n",
        "* **Spatial exploration-based CNNs** : L'idée derrière l'exploration spatiale est d'utiliser différentes tailles de noyau afin d'explorer différents niveaux de caractéristiques visuelles dans les données d'entrée.\n",
        "\n",
        "* **Depth-based CNNs** La profondeur désigne ici la profondeur du réseau de neurones, c'est-à-dire le nombre de couches. Ainsi, l'idée ici est de créer un modèle CNN avec plusieurs couches convolutives afin d'extraire des caractéristiques visuelles très complexes.\n",
        "\n",
        "* **Width-based CNNs** La largeur fait référence au nombre de canaux ou de cartes de caractéristiques dans les données ou les caractéristiques extraites des données. Ainsi, les CNN basés sur la largeur consistent à augmenter le nombre de cartes de caractéristiques au fur et à mesure que nous passons des couches d'entrée aux couches de sortie.\n",
        "\n",
        "* **Multi-path-based CNNs** Jusqu'à présent, les trois types d'architectures précédents présentent une monotonie dans les connexions entre couches, c'est-à-dire que les connexions directes n'existent qu'entre couches consécutives. Les CNN multi-chemins ont apporté l'idée de créer des connexions de raccourci ou de sauter des connexions entre des couches non consécutives.\n",
        "\n",
        "Un avantage clé des architectures multi-chemins est un meilleur flux d'informations à travers plusieurs couches, grâce aux connexions sautées. Ceci, à son tour, permet également au gradient de revenir aux couches d'entrée sans trop de dissipation. \n",
        "\n",
        "Après avoir examiné les différentes configurations architecturales trouvées dans les modèles CNN, nous allons maintenant examiner comment les CNN ont évolué au fil des ans depuis. ils ont d'abord été utilisés\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY20xhiQxY-B"
      },
      "source": [
        "## Evolution of CNN architectures\n",
        "\n",
        "Les CNN existent depuis 1989, lorsque le premier CNN multicouche, appelé ConvNet, a été développé par Yann LeCun. Ce modèle pourrait effectuer des tâches de cognition visuelle telles que l'identification de chiffres manuscrits. En 1998, LeCun a développé un modèle ConvNet amélioré appelé LeNet. En raison de sa grande précision dans les tâches de reconnaissance optique, LeNet a été adopté pour une utilisation industrielle peu après son invention. Depuis lors, les CNN sont l'un des modèles d'apprentissage automatique les plus réussis, tant dans l'industrie que dans les universités. Le diagramme suivant montre une brève chronologie des développements architecturaux au cours de la vie des CNN, de 1989 à 2020.\n",
        "\n",
        "<img src='https://static.packt-cdn.com/products/9781789614381/graphics/image/B12158_03_05.jpg' width=500px>\n",
        "\n",
        "\n",
        "Comme nous pouvons le voir, il existe un écart important entre les années 1998 et 2012. Cela est principalement dû au fait qu'il n'y avait pas un ensemble de données suffisamment grand et approprié pour démontrer les capacités des CNN, en particulier les CNN profonds. Et sur les petits ensembles de données existants de l'époque, tels que MNIST, les modèles d'apprentissage automatique classiques tels que les SVM commençaient à battre les performances de CNN. Au cours de ces années, quelques développements de CNN ont eu lieu.\n",
        "\n",
        "La fonction d'activation ReLU a été conçue pour traiter le problème d'explosion et de décroissance du gradient lors de la rétropropagation. L'initialisation non aléatoire des valeurs des paramètres du réseau s'est avérée cruciale. La mise en commun maximale a été inventée comme méthode efficace de sous-échantillonnage. Les GPU devenaient populaires pour la formation des réseaux de neurones, en particulier les CNN à grande échelle. Enfin, et surtout, un ensemble de données dédié à grande échelle d'images annotées appelé ImageNet (http://www.image-net.org/) a été créé par un groupe de recherche à Stanford. Cet ensemble de données est toujours l'un des principaux ensembles de données d'analyse comparative pour les modèles CNN à ce jour.\n",
        "\n",
        "Avec tous ces développements s'accumulant au fil des ans, en 2012, une conception architecturale différente a entraîné une amélioration massive des performances de CNN sur l'ensemble de données ImageNet. Ce réseau s'appelait AlexNet (nommé d'après le créateur, Alex Krizhevsky). AlexNet, en plus d'avoir divers aspects novateurs tels que le recadrage aléatoire et le pré-entraînement, a établi la tendance à la conception de couches convolutives uniformes et modulaires. La structure de couche uniforme et modulaire a été avancée en empilant à plusieurs reprises de tels modules (de couches convolutives), résultant en des CNN très profonds également connus sous le nom de VGG.\n",
        "\n",
        "Une autre approche consistant à brancher les blocs/modules de couches convolutives et à empiler ces blocs branchés les uns sur les autres s'est avérée extrêmement efficace pour des tâches visuelles personnalisées. Ce réseau s'appelait GoogLeNet (comme il a été développé chez Google) ou Inception v1 (inception étant le terme pour ces blocs ramifiés). Plusieurs variantes des réseaux VGG et Inception ont suivi, telles que VGG16, VGG19, Inception v2, Inception v3, etc.\n",
        "\n",
        "La phase suivante de développement a commencé avec des connexions par saut. Pour résoudre le problème de la décroissance du gradient lors de la formation des CNN, des couches non consécutives ont été connectées via des connexions de saut de peur que les informations ne se dissipent pas  entre elles en raison de petits gradients. Un type de réseau populaire qui a émergé avec cette astuce, parmi d'autres caractéristiques nouvelles telles que la normalisation par lots, était ResNet.\n",
        "\n",
        "Une extension logique de ResNet était DenseNet, où les couches étaient densément connectées les unes aux autres, c'est-à-dire que chaque couche reçoit l'entrée de toutes les cartes de caractéristiques de sortie des couches précédentes. De plus, des architectures hybrides ont ensuite été développées en mélangeant des architectures réussies du passé telles que Inception-ResNet et ResNeXt, où les branches parallèles au sein d'un bloc ont été multipliées.\n",
        "\n",
        "Dernièrement, la technique d'amplification des canaux s'est avérée utile pour améliorer les performances de CNN. L'idée ici est d'apprendre de nouvelles fonctionnalités et d'exploiter des fonctionnalités pré-appris grâce à l'apprentissage par transfert. Plus récemment, la conception automatique de nouveaux blocs et la recherche d'architectures CNN optimales ont été une tendance croissante dans la recherche CNN. Des exemples de tels CNN sont MnasNets et EfficientNets. L'approche derrière ces modèles consiste à effectuer une recherche d'architecture neuronale pour en déduire une architecture CNN optimale avec une approche de mise à l'échelle de modèle uniforme.\n",
        "\n",
        "Dans la section suivante, nous reviendrons sur l'un des premiers modèles CNN et examinerons de plus près les différentes architectures CNN développées depuis. Nous construirons ces architectures à l'aide de PyTorch, en entraînant certains des modèles sur des ensembles de données du monde réel. Nous explorerons également le référentiel de modèles CNN pré-entraînés de PyTorch, communément appelé model-zoo. Nous apprendrons à affiner ces modèles pré-entraînés ainsi qu'à exécuter des prédictions sur eux.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZOGxDKe26Sy"
      },
      "source": [
        "## LeNet\n",
        "\n",
        "LeNet, connu à l'origine sous le nom de LeNet-5, est l'un des premiers modèles CNN, développé en 1998. Le nombre 5 dans LeNet-5 représente le nombre total de couches dans ce modèle, c'est-à-dire deux couches convolutives et trois couches entièrement connectées. Avec environ 60 000 paramètres au total, ce modèle a donné des performances de pointe sur les tâches de reconnaissance d'images pour les images numériques manuscrites en 1998. Comme prévu à partir d'un modèle CNN, LeNet a démontré l'invariance de rotation, de position et d'échelle ainsi que la robustesse contre la distorsion des images. Contrairement aux modèles classiques d'apprentissage automatique de l'époque, tels que les SVM, qui traitaient chaque pixel de l'image séparément, LeNet exploitait la corrélation entre les pixels voisins\n",
        "\n",
        "Notez que bien que LeNet ait été développé pour la reconnaissance de chiffres manuscrits, il peut certainement être étendu à d'autres tâches de classification d'imagese. Le schéma suivant montre l'architecture d'un modèle LeNet\n",
        "\n",
        "<img src='https://www.topbots.com/wp-content/uploads/2020/02/LeNet5_800px_web.jpg' width=700px>\n",
        "\n",
        "\n",
        "Comme mentionné précédemment, il existe deux couches convolutives suivies les trois couches entièrement connectées (y compris la couche de sortie). Cette approche consistant à empiler des couches convolutives suivies de couches entièrement connectées est devenue plus tard une tendance dans la recherche CNN et est toujours appliquée aux derniers modèles CNN. Outre ces couches, il existe des couches de pooling entre les deux. Il s'agit essentiellement de couches de sous-échantillonnage qui réduisent la taille spatiale de la représentation de l'image, réduisant ainsi le nombre de paramètres et de calculs. La couche de pooling utilisée dans LeNet était un polling  moyen qui avait des poids pouvant être entraînés. Peu de temps après, le pooling  maximale est devenue la fonction de pooling la plus couramment utilisée dans les CNN.\n",
        "\n",
        "Les nombres en bas de  chaque couche de la figure indiquent les dimensions (pour les couches d'entrée, de sortie et entièrement connectées) ou la taille de la fenêtre (pour les couches convolutives et de pooling). L'entrée attendue pour une image en niveaux de gris est de 32x32 pixels. Cette image est ensuite exploitée par des noyaux convolutifs 5x5, suivis d'un pooling 2x2, et ainsi de suite. La taille de la couche de sortie est de 10, représentant les 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_80UwrOk3DGI"
      },
      "source": [
        "from torch import nn\n",
        "class LeNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 3 input image channel, 6 output feature maps and 5x5 conv kernel\n",
        "        self.cn1 = nn.Conv2d(3, 6, 5)\n",
        "        # 6 input image channel, 16 output feature maps and 5x5 conv kernel\n",
        "        self.cn2 = nn.Conv2d(6, 16, 5)\n",
        "        # fully connected layers of size 120, 84 and 10\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 is the spatial dimension at this layer\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution with 5x5 kernel\n",
        "        x = F.relu(self.cn1(x))\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(x, (2, 2))\n",
        "        # Convolution with 5x5 kernel\n",
        "        x = F.relu(self.cn2(x))\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(x, (2, 2))\n",
        "        # Flatten spatial and depth dimensions into a single vector\n",
        "        x = x.view(-1, self.flattened_features(x))\n",
        "        # Fully connected operations\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def flattened_features(self, x):\n",
        "        # all except the first (batch) dimension\n",
        "        size = x.size()[1:]  \n",
        "        num_feats = 1\n",
        "        for s in size:\n",
        "            num_feats *= s\n",
        "        return num_feats\n",
        "\n",
        "\n",
        "lenet = LeNet()\n",
        "print(lenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDzLs0iE3f4y"
      },
      "source": [
        "## Fine-tuning the AlexNet model\n",
        "Dans cette section, nous allons d'abord jeter un coup d'œil rapide à l'architecture AlexNet et comment en créer une en utilisant PyTorch. Ensuite, nous explorerons le référentiel de modèles CNN pré-entraînés de PyTorch.\n",
        "\n",
        "AlexNet est un successeur de LeNet avec des modifications incrémentielles de l'architecture, telles que 8 couches (5 convolutives et 3 entièrement connectées) au lieu de 5, et 60 millions de paramètres de modèle au lieu de 60 000, ainsi que l'utilisation de MaxPool au lieu d'AvgPool. De plus, AlexNet a été formé et testé sur un ensemble de données beaucoup plus volumineux - ImageNet, qui fait plus de 100 Go, par opposition à l'ensemble de données MNIST (sur lequel LeNet a été formé), qui représente quelques Mo. AlexNet a véritablement révolutionné les CNN car il est devenu une classe de modèles considérablement plus puissante sur les tâches liées à l'image que les autres modèles d'apprentissage automatique classiques, tels que les SVM. \n",
        "\n",
        "\n",
        "<img src='https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png' width=700px>\n",
        "\n",
        "\n",
        "Comme nous pouvons le voir, l'architecture suit le thème commun de LeNet d'avoir des couches convolutives empilées séquentiellement, suivies d'une série de couches entièrement connectées vers l'extrémité de sortie. PyTorch facilite la traduction d'une telle architecture de modèle en code réel. Cela peut être vu dans l'équivalent de code PyTorch suivant de l'architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Me9Badu4RoU"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, number_of_classes):\n",
        "    super(AlexNet, self).__init__()\n",
        "    self.feats = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=5),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                               nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, padding=2),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                               nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(kernel_size=2, stride=2),)\n",
        "    self.clf = nn.Linear(in_features=256, out_features=number_of_classes)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "      op = self.feats(inp)\n",
        "      op = op.view(op.size(0), -1)\n",
        "      op = self.clf(op)\n",
        "      return op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Otq8TAB5Hp9"
      },
      "source": [
        "Mais outre la possibilité d'initialiser l'architecture du modèle et de l'entraîner nous-mêmes, PyTorch, avec son package torchvision, fournit un sous-package de modèles, qui contient des définitions de modèles CNN destinés à résoudre différentes tâches, telles que la classification d'images. , segmentation sémantique, détection d'objets, etc. Voici une liste des modèles disponibles pour la tâche de classification d'images (source : https://pytorch.org/docs/stable/torchvision/models.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gepVV1-51Yi"
      },
      "source": [
        "model_finetune = models.alexnet(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzfJWJA054ep"
      },
      "source": [
        "print(model_finetune.features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF_exTE06Ffy"
      },
      "source": [
        "print(model_finetune.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEdCv2lt59aQ"
      },
      "source": [
        "# change the last layer from 1000 classes to 2 classes\n",
        "model_finetune.classifier[6] = nn.Linear(4096, len(classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT1qjw2BZ0Tb"
      },
      "source": [
        "## VGG model\n",
        "\n",
        "Nous avons déjà discuté de LeNet et AlexNet, deux des architectures fondamentales de CNN. Au fur et à mesure que nous progressons dans ce chapitre, nous explorerons des modèles CNN de plus en plus complexes. Cependant, les principes clés dans la construction de ces architectures modèles seront les mêmes. Nous verrons une approche de construction de modèles modulaires consistant à assembler des couches convolutives, des couches de regroupement et des couches entièrement connectées en blocs/modules, puis à empiler ces blocs de manière séquentielle ou ramifiée. Dans cette section, nous examinons le successeur d'AlexNet – VGGNet.\n",
        "\n",
        "\n",
        "Le nom VGG est dérivé du Visual Geometry Group de l'Université d'Oxford, où ce modèle a été inventé. Comparé aux 8 couches et 60 millions de paramètres d'AlexNet, VGG se compose de 13 couches (10 couches convolutives et 3 couches entièrement connectées) et de 138 millions de paramètres. VGG empile essentiellement plus de couches sur l'architecture AlexNet avec des noyaux de convolution de plus petite taille (2x2 ou 3x3). Ainsi, la nouveauté de VGG réside dans le niveau de profondeur sans précédent qu'il apporte avec son architecture. La figure suivante montre l'architecture VGG\n",
        "\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*m1DDQfNy-vqRbrQLBfoQMw.jpeg' width=700px>\n",
        "\n",
        "\n",
        "Le sous-package torchvision.model de PyTorch fournit le modèle VGG pré-entraîné (avec six variantes ) entraîné sur l'ensemble de données ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63BxyU1GikvJ"
      },
      "source": [
        "odel_finetune = models.vgg13(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8CXQ9rpiqjS"
      },
      "source": [
        "## GoogLeNet and Inception v3\n",
        "\n",
        "Comme nous avons découvert la progression des modèles CNN de LeNet à VGG jusqu'à présent, nous avons observé l'empilement séquentiel de couches plus convolutives et entièrement connectées. Cela a résulté en des réseaux profonds avec beaucoup de paramètres à former. GoogLeNet est apparu comme un type d'architecture CNN radicalement différent, composé d'un module de couches convolutives parallèles appelé module de démarrage. Pour cette raison, GoogLeNet est également appelé Inception v1 (v1 a marqué la première version car d'autres versions sont arrivées plus tard). Certains des éléments radicalement nouveaux introduits dans GoogLeNet étaient les suivants.\n",
        "\n",
        "* **Le module de inception**- un module de plusieurs couches convolutives parallèles\n",
        "\n",
        "* Utilisation de **convolutions 1x1** pour réduire le nombre de paramètres du modèle.\n",
        "\n",
        "* **Global average pooling** au lieu d'une couche entièrement connectée - réduit le surapprentissage\n",
        "\n",
        "* Utilisation de **classificateurs auxiliaires** pour l'entraînement - pour la régularisation et la stabilité du gradient.\n",
        "\n",
        "GoogLeNet a 22 couches, ce qui est plus que le nombre de couches de n'importe quelle variante de modèle VGG. Pourtant, en raison de certaines des astuces d'optimisation utilisées, le nombre de paramètres dans GoogLeNet est de 5 millions, ce qui est bien inférieur aux 138 millions de paramètres de VGG. Développons quelques-unes des principales caractéristiques de ce modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmdBqoK5qTgo"
      },
      "source": [
        "### Inception modules\n",
        "\n",
        "La contribution la plus importante de ce modèle a peut-être été le développement d'un module convolutif avec plusieurs couches convolutives fonctionnant en parallèle, qui sont finalement concaténées pour produire un seul vecteur de sortie. Ces couches convolutives parallèles fonctionnent avec différentes tailles de noyau allant de 1x1 à 3x3 à 5x5. L'idée est d'extraire tous les niveaux d'informations visuelles de l'image. Outre ces convolutions, une couche de pooling max 3x3 ajoute un autre niveau d'extraction de caractéristiques. La figure montre le schéma fonctionnel initial ainsi que l'architecture globale de GoogLeNet :\n",
        "\n",
        "\n",
        "<img src='https://d3i71xaburhd42.cloudfront.net/45284a29fe804516e77ab56aea38c560f0fa9295/3-Figure3-1.png' width=900px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mY0FBg4toYj"
      },
      "source": [
        "En utilisant ce diagramme d'architecture, nous pouvons construire le module de création dans PyTorch comme indiqué ici :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICqtTh6rtgGW"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, input_planes, n_channels1x1, n_channels3x3red, n_channels3x3, n_channels5x5red, n_channels5x5, pooling_planes):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        # 1x1 convolution branch\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(input_planes, n_channels1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(n_channels1x1),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        " \n",
        "        # 1x1 convolution -> 3x3 convolution branch\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(input_planes, n_channels3x3red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n_channels3x3red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n_channels3x3red, n_channels3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n_channels3x3),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        " \n",
        "        # 1x1 conv -> 5x5 conv branch\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(input_planes, n_channels5x5red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n_channels5x5red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n_channels5x5red, n_channels5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n_channels5x5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n_channels5x5, n_channels5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n_channels5x5),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        " \n",
        "        # 3x3 pool -> 1x1 conv branch\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(input_planes, pooling_planes, kernel_size=1),\n",
        "            nn.BatchNorm2d(pooling_planes),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        " \n",
        "    def forward(self, ip):\n",
        "        op1 = self.block1(ip)\n",
        "        op2 = self.block2(ip)\n",
        "        op3 = self.block3(ip)\n",
        "        op4 = self.block4(ip)\n",
        "        return torch.cat([op1,op2,op3,op4], 1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pamgqGymuV4t"
      },
      "source": [
        "Ensuite, nous examinerons une autre caractéristique importante de GoogLeNet - les convolutions 1x1\n",
        "\n",
        "#### 1x1 convolutions\n",
        "\n",
        "En plus des couches convolutives parallèles dans un module de démarrage, chaque couche parallèle a une couche convolutive 1x1 précédente. La raison derrière l'utilisation de ces couches convolutives 1x1 est la réduction de la dimensionnalité. Les convolutions 1x1 ne modifient pas la largeur et la hauteur de la représentation d'image mais peuvent modifier la profondeur d'une représentation d'image. Cette astuce est utilisée pour réduire la profondeur des caractéristiques visuelles d'entrée avant d'effectuer les convolutions 1x1, 3x3 et 5x5 en parallèle. La réduction du nombre de paramètres permet non seulement de construire un modèle plus léger, mais aussi de lutter contre le surapprentissage.\n",
        "\n",
        "#### Global average pooling\n",
        "\n",
        "Si nous examinons l'architecture globale de GoogLeNet, l'avant-dernière couche de sortie du modèle est précédée d'une couche de pooling moyenne 7x7. Cette couche aide à nouveau à réduire le nombre de paramètres du modèle, réduisant ainsi le surapprentissage. Sans cette couche, le modèle aurait des millions de paramètres supplémentaires en raison des connexions denses d'une couche entièrement connectée.\n",
        "\n",
        "#### Auxiliary classifiers\n",
        "\n",
        "L'architecture de googleNet montre également deux branches de sortie supplémentaires ou auxiliaires dans le modèle. Ces classificateurs auxiliaires sont censés résoudre le problème de disparition de gradient en ajoutant à l'amplitude des gradients lors de la rétropropagation, en particulier pour les couches vers l'extrémité d'entrée. Étant donné que ces modèles comportent un grand nombre de couches, la disparition des dégradés peut devenir un goulot d'étranglement. Par conséquent, l'utilisation de classificateurs auxiliaires s'est avérée utile pour ce modèle profond à 22 couches. De plus, les branches auxiliaires aident également à la régularisation. Veuillez noter que ces branches auxiliaires sont désactivées/supprimées lors de la réalisation de prédictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhwgSSRGuBkO"
      },
      "source": [
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        " \n",
        "        self.im1 = InceptionModule(192,  64,  96, 128, 16, 32, 32)\n",
        "        self.im2 = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n",
        " \n",
        "        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        " \n",
        "        self.im3 = InceptionModule(480, 192,  96, 208, 16,  48,  64)\n",
        "        self.im4 = InceptionModule(512, 160, 112, 224, 24,  64,  64)\n",
        "        self.im5 = InceptionModule(512, 128, 128, 256, 24,  64,  64)\n",
        "        self.im6 = InceptionModule(512, 112, 144, 288, 32,  64,  64)\n",
        "        self.im7 = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n",
        " \n",
        "        self.im8 = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.im9 = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n",
        " \n",
        "        self.average_pool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(4096, 1000)\n",
        " \n",
        "    def forward(self, ip):\n",
        "        op = self.stem(ip)\n",
        "        out = self.im1(op)\n",
        "        out = self.im2(op)\n",
        "        op = self.maxpool(op)\n",
        "        op = self.a4(op)\n",
        "        op = self.b4(op)\n",
        "        op = self.c4(op)\n",
        "        op = self.d4(op)\n",
        "        op = self.e4(op)\n",
        "        op = self.max_pool(op)\n",
        "        op = self.a5(op)\n",
        "        op = self.b5(op)\n",
        "        op = self.avgerage_pool(op)\n",
        "        op = op.view(op.size(0), -1)\n",
        "        op = self.fc(op)\n",
        "        return op"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwfOrMHlxGsj"
      },
      "source": [
        "En plus d'instancier notre propre modèle, nous pouvons toujours charger un GoogLeNet pré-entraîné avec seulement deux lignes de code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLaZTz0kxQDp"
      },
      "source": [
        "import torchvision.models as models\n",
        "model = models.googlenet(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTOqzuDExdBB"
      },
      "source": [
        "Enfin, comme mentionné précédemment, un certain nombre de versions du modèle Inception ont été développées plus tard. L'un des plus éminents était Inception v3, dont nous discuterons brièvement ensuite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1D_FvtQxiTN"
      },
      "source": [
        "### Inception v3\n",
        "\n",
        "Ce successeur d'Inception v1 a un total de 24 millions de paramètres contre 5 millions dans la v1. Outre l'ajout de plusieurs couches supplémentaires, ce modèle a introduit différents types de modules de démarrage, qui sont empilés de manière séquentielle. La figure montre les différents modules de démarrage et l'architecture complète du modèle\n",
        "\n",
        "\n",
        "<img src='https://www.researchgate.net/profile/Masoud-Mahdianpari/publication/326421398/figure/fig6/AS:649353890889730@1531829440919/Schematic-diagram-of-InceptionV3-model-compressed-view.png' width=900px>\n",
        "\n",
        "Il ressort de l'architecture que ce modèle est une extension architecturale du modèle Inception v1. Encore une fois, en plus de construire le modèle manuellement, nous pouvons utiliser le modèle pré-entraîné du référentiel de PyTorch comme suit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrh0ht9rzYnm"
      },
      "source": [
        "import torchvision.models as models\n",
        "model = models.inception_v3(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iWM0VnKzipX"
      },
      "source": [
        "Dans la section suivante, nous passerons en revue les classes de modèles CNN qui ont efficacement combattu le problème de la disparition de gradient dans les CNN très profonds - ResNet et DenseNet. Nous découvrirons les nouvelles techniques de saut de connexions et de connexions denses et utiliserons PyTorch pour coder les modules fondamentaux derrière ces architectures avancées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrLyvGMyGUcC"
      },
      "source": [
        "## Discussing ResNet and DenseNet architectures\n",
        "\n",
        "Dans la section précédente, nous avons exploré les modèles Inception, qui avaient un nombre réduit de paramètres de modèle à mesure que le nombre de couches augmentait, grâce aux convolutions 1x1 et à la mise en commun moyenne globale. De plus, des classificateurs auxiliaires ont été utilisés pour lutter contre le problème de la disparition gradient.\n",
        "\n",
        "ResNet a introduit le concept de saut de connexion. Cette astuce simple mais efficace surmonte le problème du dépassement des paramètres et de la disparition de gradients. L'idée, comme le montre le schéma suivant, est assez simple. L'entrée est d'abord passée par une transformation non linéaire (convolutions suivies d'activations non linéaires) puis la sortie de cette transformation (appelée résidu) est ajoutée à l'entrée d'origine. Chaque bloc de ce calcul est appelé bloc résiduel, d'où le nom du modèle - réseau résiduel ou ResNet.\n",
        "\n",
        "\n",
        "<img src='https://forums.fast.ai/uploads/default/9899294137a145f829305ca499cf03a999f3ef3d' width=300px>\n",
        "\n",
        "En utilisant ces connexions sautées (ou raccourcies), le nombre de paramètres est limité à 26 millions de paramètres pour un total de 50 couches (ResNet-50). En raison du nombre limité de paramètres, ResNet a pu bien généraliser sans surcharger même lorsque le nombre de couches est augmenté à 152 (ResNet-152). Le schéma suivant montre l'architecture ResNet-50\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://www.aimspress.com/aimspress-data/mbe/2020/5/PIC/mbe-17-05-328-g007.jpg' width=800px>\n",
        "\n",
        "Il existe deux types de blocs résiduels - convolutifs et d'identité, tous deux ayant des connexions de saut. Pour le bloc convolutif, une couche convolutive 1x1 est ajoutée, ce qui contribue encore à réduire la dimensionnalité. Un bloc résiduel pour ResNet peut être implémenté dans PyTorch comme indiqué ici\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0L7bRs91vIg"
      },
      "source": [
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    multiplier=1\n",
        "    def __init__(self, input_num_planes, num_planes, strd=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=input_num_planes, out_channels=num_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(num_planes)\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=num_planes, out_channels=num_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(num_planes)\n",
        " \n",
        "        self.res_connnection = nn.Sequential()\n",
        "        if strd > 1 or input_num_planes != self.multiplier*num_planes:\n",
        "            self.res_connnection = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=input_num_planes, out_channels=self.multiplier*num_planes, kernel_size=1, stride=strd, bias=False),\n",
        "                nn.BatchNorm2d(self.multiplier*num_planes)\n",
        "            )\n",
        "    def forward(self, inp):\n",
        "        op = F.relu(self.batch_norm1(self.conv_layer1(inp)))\n",
        "        op = self.batch_norm2(self.conv_layer2(op))\n",
        "        op += self.res_connnection(inp)\n",
        "        op = F.relu(op)\n",
        "        return op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLJeKH01_PR"
      },
      "source": [
        "Pour démarrer rapidement avec ResNet, nous pouvons toujours utiliser le modèle ResNet pré-entraîné du référentiel de PyTorch :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jxzeDTJ2BTq"
      },
      "source": [
        "import torchvision.models as models\n",
        "model = models.resnet50(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPYnvU4y2KRJ"
      },
      "source": [
        "ResNet utilise la fonction d'identité (en connectant directement l'entrée à la sortie) pour préserver le gradient pendant la rétropropagation (car le gradient sera de 1). Pourtant, pour les réseaux extrêmement profonds, ce principe peut ne pas suffire pour préserver de forts gradients de la couche de sortie vers la couche d'entrée.\n",
        "\n",
        "Le modèle CNN dont nous discuterons ensuite est conçu pour assurer un fort gradient de flux, ainsi qu'une réduction supplémentaire du nombre de paramètres requis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4uOhAAO65NJ"
      },
      "source": [
        "## DenseNet\n",
        "\n",
        "Les connexions de saut de ResNet connectaient l'entrée d'un bloc résiduel directement à sa sortie. Cependant, la connexion entre les blocs résiduels est toujours séquentielle, c'est-à-dire que le bloc résiduel numéro 3 a une connexion directe avec le bloc 2 mais pas de connexion directe avec le bloc 1.\n",
        "\n",
        "DenseNet, ou réseaux denses, a introduit l'idée de connecter chaque couche convolutive à chaque autre couche au sein de ce qu'on appelle un bloc dense. Et chaque bloc dense est connecté à chaque autre bloc dense du DenseNet global. Un bloc dense est simplement un module de deux couches convolutives 3x3 densément connectées.\n",
        "\n",
        "Ces connexions denses garantissent que chaque couche reçoit des informations de toutes les couches précédentes du réseau. Cela garantit qu'il y a un fort gradient d'écoulement de la dernière couche jusqu'à la toute première couche. Contre-intuitivement, le nombre de paramètres d'un tel paramètre de réseau sera également faible. Comme chaque couche reçoit les cartes de caractéristiques de toutes les couches précédentes, le nombre requis de canaux (profondeur) peut être inférieur. Dans les modèles précédents, la profondeur croissante représentait l'accumulation d'informations provenant des couches précédentes, mais nous n'en avons plus besoin, grâce aux connexions denses partout dans le réseau.\n",
        "\n",
        "\n",
        "Une différence clé entre ResNet et DenseNet est également que, dans ResNet, l'entrée a été ajoutée à la sortie à l'aide de connexions sautées. Mais dans le cas de DenseNet, les sorties des couches précédentes sont concaténées avec la sortie de la couche actuelle. Et la concaténation se produit dans la dimension de profondeur\n",
        "\n",
        "Cela pourrait soulever une question sur l'explosion de la taille des sorties à mesure que nous progressons dans le réseau. Pour lutter contre cet effet aggravant, un type spécial de bloc appelé bloc de transition est conçu pour ce réseau. Composé d'une couche convolutive 1x1 suivie d'une couche de mise en commun 2x2, ce bloc normalise ou réinitialise la taille de la dimension de profondeur afin que la sortie de ce bloc puisse ensuite être transmise au(x) bloc(s) dense(s) suivant(s). Le schéma suivant montre l'architecture DenseNet.\n",
        "\n",
        "<img src='https://www.researchgate.net/profile/Jian-Guan-24/publication/332522654/figure/fig3/AS:749240657661954@1555644301332/Framework-of-Densenet-which-contains-3-Dense-blocks-and-each-block-contains-16.ppm' width=500px>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1DIRTE1Eku3"
      },
      "source": [
        "\n",
        "Comme mentionné précédemment, il existe deux types de blocs impliqués - le bloc dense et le bloc de transition. Ces blocs peuvent être écrits en tant que classes dans PyTorch en quelques lignes de code, comme indiqué ici"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvMsT2aSEFL9"
      },
      "source": [
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, input_num_planes, rate_inc):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm2d(input_num_planes)\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=input_num_planes, out_channels=4*rate_inc, kernel_size=1, bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(4*rate_inc)\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=4*rate_inc, out_channels=rate_inc, kernel_size=3, padding=1, bias=False)\n",
        "    def forward(self, inp):\n",
        "        op = self.conv_layer1(F.relu(self.batch_norm1(inp)))\n",
        "        op = self.conv_layer2(F.relu(self.batch_norm2(op)))\n",
        "        op = torch.cat([op,inp], 1)\n",
        "        return op\n",
        "\n",
        "class TransBlock(nn.Module):\n",
        "    def __init__(self, input_num_planes, output_num_planes):\n",
        "        super(TransBlock, self).__init__()\n",
        "        self.batch_norm = nn.BatchNorm2d(input_num_planes)\n",
        "        self.conv_layer = nn.Conv2d(in_channels=input_num_planes, out_channels=output_num_planes, kernel_size=1, bias=False)\n",
        "    def forward(self, inp):\n",
        "        op = self.conv_layer(F.relu(self.batch_norm(inp)))\n",
        "        op = F.avg_pool2d(op, 2)\n",
        "        return op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w28r74XkGIpo"
      },
      "source": [
        "Ces blocs sont ensuite empilés de manière dense pour former l'architecture globale DenseNet. DenseNet, comme ResNet, est disponible dans des variantes telles que DenseNet121, DenseNet161, DenseNet169 et DenseNet201, où les nombres représentent le nombre total de couches. Un si grand nombre de couches est obtenu par l'empilement répété des blocs denses et de transition plus une couche convolutive 7x7 fixe à l'extrémité d'entrée et une couche entièrement connectée xe à l'extrémité de sortie. PyTorch fournit des modèles pré-entraînés pour toutes ces variantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lp1qhfa2lGi"
      },
      "source": [
        "mport torchvision.models as models\n",
        "\n",
        "densenet121 = models.densenet121(pretrained=True)\n",
        "densenet161 = models.densenet161(pretrained=True)\n",
        "densenet169 = models.densenet169(pretrained=True)\n",
        "densenet201 = models.densenet201(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSjiSG8a2xe_"
      },
      "source": [
        "DenseNet surpasse tous les modèles discutés jusqu'à présent sur l'ensemble de données ImageNet. Divers modèles hybrides ont été développés en mélangeant et en faisant correspondre les idées présentées dans les sections précédentes. Les modèles Inception-ResNet et ResNeXt sont des exemples de tels réseaux hybrides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyES3nxy2-KY"
      },
      "source": [
        "Dans la section suivante, nous allons examiner les architectures CNN actuelles les plus performantes – EffcientNets. Nous discuterons également de l'avenir du développement architectural de CNN tout en abordant l'utilisation des architectures de CNN pour des tâches au-delà de la classification d'images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_APLaWAV4XmA"
      },
      "source": [
        "## Understanding EffcientNets and the future of CNN architectures.\n",
        "\n",
        "Jusqu'à présent, dans notre exploration de LeNet à DenseNet, nous avons remarqué un thème sous-jacent dans l'avancement des architectures CNN. Le thème principal est l'expansion ou la mise à l'échelle du modèle CNN via l'un des éléments suivants.\n",
        "\n",
        "* Une augmentation du nombre de couches.\n",
        "* Une augmentation du nombre de cartes de caractéristiques ou de canaux dans une couche convolutive\n",
        "\n",
        "* Une augmentation de la dimension spatiale passant des images 32x32 pixels dans LeNet à des images 224x224 pixels dans AlexNet et ainsi de suite\n",
        "\n",
        "Ces trois aspects différents sur lesquels la mise à l'échelle peut être effectuée sont respectivement identifiés comme la profondeur, la largeur et la résolution. Au lieu de mettre à l'échelle manuellement ces attributs, ce qui conduit souvent à des résultats sous-optimaux, EffcientNets utilise la recherche d'architecture neuronale pour calculer les facteurs de mise à l'échelle optimaux pour chacun d'eux.\n",
        "\n",
        "L'augmentation de la profondeur est jugée importante car plus le réseau est profond, plus le modèle est complexe et, par conséquent, il peut apprendre des fonctionnalités très complexes. Cependant, il y a un compromis parce que, avec l'augmentation de la profondeur, le problème de disparition de gradient s'aggrave avec le problème général du surapprentissage.\n",
        "\n",
        "De même, augmenter la largeur devrait théoriquement aider, car avec un plus grand nombre de canaux, le réseau devrait apprendre des fonctionnalités plus fines. Cependant, pour les modèles extrêmement larges, la précision a tendance à saturer rapidement.\n",
        "\n",
        "Enfin, les images à plus haute résolution devraient en théorie mieux fonctionner car elles contiennent des informations plus fines. Empiriquement, cependant, l'augmentation de la résolution ne produit pas une augmentation linéairement équivalente des performances du modèle. Tout cela pour dire qu'il y a des compromis à faire tout en décidant des facteurs d'échelle et, par conséquent, la recherche d'architecture neuronale aide à trouver les facteurs d'échelle optimaux.\n",
        "\n",
        "EfficientNet propose de trouver l'architecture qui a le bon équilibre entre profondeur, largeur et résolution, et ces trois aspects sont mis à l'échelle ensemble à l'aide d'un facteur d'échelle global. L'architecture EfficientNet est construite en deux étapes. Tout d'abord, une architecture de base (appelée réseau de base) est conçue en fixant le facteur d'échelle à 1. À ce stade, l'importance relative de la profondeur, de la largeur et de la résolution est décidée pour la tâche et l'ensemble de données donnés. Le réseau de base obtenu est assez similaire à une architecture CNN bien connue – MnasNet, abréviation de Mobile Neural Architecture Search Network. PyTorch propose le modèle MnasNet pré-entraîné, qui peut être chargé comme indiqué ici"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8IGtNe_cDwH"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "model = models.mnasnet1_0()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-8X4GuFcmSR"
      },
      "source": [
        "Une fois le réseau de base obtenu dans la première étape, le facteur d'échelle global optimal est ensuite calculé dans le but de maximiser la précision du modèle et de minimiser le nombre de calculs. Le réseau de base est appelé EcientNet B0 et les réseaux suivants dérivés pour différents facteurs d'échelle optimaux sont appelés EcientNet B1-B7.\n",
        "\n",
        "À mesure que nous avançons, la mise à l'échelle efficace de l'architecture CNN sera une direction de recherche importante, ainsi que le développement de modules plus sophistiqués inspirés des modules de inception, résiduels et denses. Un autre aspect du développement de l'architecture CNN est de minimiser la taille du modèle tout en conservant les performances. Les MobileNets (https://pytorch.org/hub/pytorch_vision_mobilenet_v2/) sont un excellent exemple et il y a beaucoup de recherches en cours sur ce front.\n",
        "\n",
        "Les CNN sont un énorme sujet d'étude en eux-mêmes. Dans cet article, nous avons abordé le développement architectural des CNN, principalement dans le contexte de la classification d'images. Cependant, ces mêmes architectures sont utilisées dans une grande variété d'applications. Un exemple bien connu est l'utilisation de ResNets pour la détection et la segmentation d'objets sous la forme de RCNN (https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks). Certaines des variantes améliorées des RCNN sont Faster R-CNN, Mask-RCNN et Keypoint-RCNN. PyTorch fournit des modèles pré-entraînés pour les trois variantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur1BS3DFihet"
      },
      "source": [
        "faster_rcnn = models.detection.fasterrcnn_resnet50_fpn()\n",
        "mask_rcnn = models.detection.maskrcnn_resnet50_fpn()\n",
        "keypoint_rcnn = models.detection.keypointrcnn_resnet50_fpn()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9lL5u21ip9u"
      },
      "source": [
        "PyTorch fournit également des modèles pré-entraînés pour les ResNets qui sont appliqués aux tâches liées à la vidéo telles que la classification vidéo. Deux de ces modèles basés sur ResNet utilisés pour la classification vidéo sont ResNet3D et ResNet Mixed Convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YdrBJKkixSg"
      },
      "source": [
        "resnet_3d = models.video.r3d_18()\n",
        "resnet_mixed_conv = models.video.mc3_18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7T-cdJji5Al"
      },
      "source": [
        "Bien que nous ne couvrons pas en détail ces différentes applications et les modèles CNN correspondants dans cet article, nous vous encourageons à en savoir plus à leur sujet. Le site Web de PyTorch peut être un bon point de départ : https://pytorch.org/docs/stable/torchvision/models.html#object-detection-instance-segmentation-and-person-keypoint-detection"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4734bc67a20a46a9898debd187a2d7bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c238c56a37ae40fda1f66efc4deeb3ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a24825061d5450891e976326117d5f9",
              "IPY_MODEL_8ae62b4ea0f44304a059c4acce1e991d",
              "IPY_MODEL_8a5e63bd511247bb83f3c03a2937c65e"
            ]
          }
        },
        "c238c56a37ae40fda1f66efc4deeb3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a24825061d5450891e976326117d5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6279e455b6c84080bcbe49aef138887b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2b7240c165e14bb494fe1c11369ec73c"
          }
        },
        "8ae62b4ea0f44304a059c4acce1e991d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a00f534f65724cddbbedbc5bae5f5b7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ddd9303c076452b95cff547c9ba204a"
          }
        },
        "8a5e63bd511247bb83f3c03a2937c65e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c46bb09d2a244f20842419248c07c6d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:04&lt;00:00, 132MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21cf43f5f31d43199d6873b9e3fd2f04"
          }
        },
        "6279e455b6c84080bcbe49aef138887b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2b7240c165e14bb494fe1c11369ec73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a00f534f65724cddbbedbc5bae5f5b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ddd9303c076452b95cff547c9ba204a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c46bb09d2a244f20842419248c07c6d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21cf43f5f31d43199d6873b9e3fd2f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs1c91ePhhUi"
      },
      "source": [
        "# Working details of SSD\n",
        "\n",
        "Jusqu'à présent, nous avons vu un scénario dans lequel nous avons fait des prédictions après avoir progressivement convolé et fait de pooling à la sortie de la couche précédente. Cependant, nous savons que différentes couches ont des champs récepteurs différents de l'image d'origine. Par exemple, les couches initiales ont un champ récepteur plus petit par rapport aux couches finales, qui ont un champ récepteur plus grand. Ici, nous allons apprendre comment le SSD tire parti de ce phénomène pour proposer une prédiction de cadres de délimitation pour les images.\n",
        "\n",
        "Le fonctionnement derrière la façon dont SSD aide à surmonter le problème de la détection d'objets à différentes échelles est le suivant:\n",
        "* \n",
        "Nous exploitons le réseau VGG pré-entraîné et l'étendons avec quelques couches supplémentaires jusqu'à ce que nous obtenions un bloc 1 x 1\n",
        "\n",
        "* Au lieu d'exploiter uniquement la couche finale pour les prédictions de boîte englobante et de classe, nous tirerons parti de toutes les dernières couches pour faire des prédictions de classe et de boîte englobante.\n",
        "\n",
        "* À la place des boîtes d'ancrage, nous proposerons des boîtes par défaut qui ont un ensemble spécifique d'échelle et de proportions.\n",
        "\n",
        "\n",
        "* Chacune des boîtes par défaut doit prédire le décalage de l'objet et du cadre de délimitation, tout comme la façon dont les boîtes d'ancrage sont censées prédire les classes et les décalages dans YOLO\n",
        "\n",
        "Maintenant que nous comprenons les principales différences entre SSD et YOLO (c'est-à-dire que les boîtes par défaut dans SSD remplacent les boîtes d'ancrage dans YOLO et que plusieurs couches sont connectées à la couche finale dans SSD, au lieu d'un pool de convolution progressif dans YOLO), apprenons ce qui suit:\n",
        "\n",
        "* L'architecture réseau du SSD\n",
        "* Comment tirer parti de différentes couches pour les prédictions de cadre de délimitation et de classe\n",
        "* Comment attribuer une échelle et des proportions pour les boîtes par défaut dans différentes couches.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-981-15-3270-2_33/MediaObjects/482669_1_En_33_Fig1_HTML.png' width=700px>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1_3ZBtdlmh7"
      },
      "source": [
        "Comme vous pouvez le voir dans le schéma précédent, nous prenons une image de taille 300 x 300 x 3 et la faisons passer à travers un réseau VGG-16 pré-entraîné pour obtenir la sortie de la couche conv5_3. De plus, nous étendons le réseau en ajoutant quelques convolutions supplémentaires à la sortie conv5_3.\n",
        "\n",
        "Ensuite, nous obtenons un décalage de boîte englobante et une prédiction de classe pour chaque cellule et chaque boîte par défaut (plus sur les boîtes par défaut dans la section suivante ; pour l'instant, imaginons que cela soit similaire à une boîte d'ancrage). Le nombre total de prédictions provenant de la sortie conv5_3 est de 38 x 38 x 4, où 38 x 38 est la forme de sortie de la couche conv5_3 et 4 est le nombre de boîtes par défaut fonctionnant sur la couche conv5_3.\n",
        "\n",
        "Voyons maintenant les différentes échelles et proportions des boîtes par défaut. Nous allons commencer par les échelles, puis passer aux proportions.\n",
        "\n",
        "Imaginons un scénario où l'échelle minimale d'un objet est de 20 % de la hauteur et 20 % de la largeur d'une image, et l'échelle maximale de l'objet est de 90 % de la hauteur et 90 % de la largeur. Dans un tel scénario, nous augmentons progressivement l'échelle entre les couches (à mesure que nous progressons vers les couches ultérieures, la taille de l'image diminue considérablement), comme suit:\n",
        "<img src='https://lilianweng.github.io/lil-log/assets/images/SSD-box-scales.png' width=700px>\n",
        "\n",
        "\n",
        "La formule qui permet la mise à l'échelle progressive de l'image est la suivante:\n",
        "\n",
        "$$lavel \\ index:=1,...,L$$\n",
        "\n",
        "$$scale\\ of\\ boxes: s_l=s_{min}+\\frac{s_{max}-s_{smin}}{L-1}(l-1)$$\n",
        "\n",
        "Maintenant que nous comprenons comment calculer l'échelle entre les couches, nous allons maintenant apprendre à créer des boîtes de différents rapports d'aspect.\n",
        "\n",
        "Les rapports d'aspect possibles sont les suivants\n",
        "\n",
        "$$aspect\\ ratio: r\\in \\{1,2,3,0.5,0.33\\}$$\n",
        "\n",
        "Le centre de la boîte pour les différentes couches est le suivant :\n",
        "\n",
        "$$ center\\ location :(x_l^i, y_l^i)=(\\frac{i+0.5}{m},\\frac{j+0.5}{n}$$ \n",
        "\n",
        "Ici i et j représentent ensemble une cellule de la couche l\n",
        "\n",
        "La largeur et la hauteur correspondant aux différents rapports d'aspect sont calculées comme suit:\n",
        "\n",
        "$$widht: w_l^r =s_l\\sqrt{r}$$\n",
        "$$height: h_l^r =s_l\\sqrt{r}$$\n",
        "\n",
        "Notez que nous considérions quatre boîtes dans certaines couches et six boîtes dans une autre couche. Maintenant, si nous voulons avoir quatre cases, nous supprimons les proportions {3,1/3}, sinon nous considérons toutes les six cases possibles (cinq cases avec la même échelle et une case avec une échelle différente). Alors, apprenons comment nous obtenons la sixième case\n",
        "\n",
        "$$additional\\ scal: s_l^{'}\\sqrt{s_ls_{l+1}}\\ when\\ r=1$$\n",
        "\n",
        "Maintenant que nous avons toutes les cases possibles, comprenons comment nous préparons l'ensemble de données d'entraînement. Les cases par défaut qui ont une IoU supérieure à un seuil (disons, 0,5) sont considérées comme des correspondances positives, et les autres sont des correspondances négatives. Dans la sortie du SSD, nous prédisons la probabilité que la boîte appartienne à une classe (où la 0ème classe représente le fond) ainsi que le décalage de la vérité  par rapport à la boîte par défaut. Enfin, nous entraînons le modèle en optimisant les valeurs de pertes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVbGgIuZraFY"
      },
      "source": [
        "## Components in SSD code\n",
        "\n",
        "\n",
        "Les fonctions utilitaires principales de cette section sont présentes dans le référentiel GitHub : https://github.com/sizhky/ssd-utils/  . Apprenons-les un par un avant de commencer le processus de formation\n",
        "\n",
        "Il y a trois fichiers dans le référentiel GitHub. Examinons-les un peu et comprenons-les avant de nous entraîner. Notez que cette section ne fait pas partie du processus de formation, mais sert plutôt à comprendre les importations utilisées pendant la formation. Nous importons les classes SSD300 et MultiBoxLoss depuis le fichier model.py dans le référentiel GitHub. Apprenons-en tous les deux\n",
        "\n",
        "### SSD300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnNThOBxsLHF"
      },
      "source": [
        "class SSD300(nn.Module):\n",
        "    \"\"\"\n",
        "    The SSD300 network - encapsulates the base VGG network, auxiliary, and prediction convolutions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes, device):\n",
        "        super(SSD300, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.device = device\n",
        "        self.base = VGGBase()\n",
        "        self.aux_convs = AuxiliaryConvolutions()\n",
        "        self.pred_convs = PredictionConvolutions(n_classes)\n",
        "\n",
        "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
        "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "        # Prior boxes\n",
        "        self.priors_cxcy = self.create_prior_boxes()\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
        "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
        "        \"\"\"\n",
        "        # Run VGG base network convolutions (lower level feature map generators)\n",
        "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
        "\n",
        "        # Rescale conv4_3 after L2 norm\n",
        "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
        "        # (PyTorch autobroadcasts singleton dimensions during arithmetic)\n",
        "\n",
        "        # Run auxiliary convolutions (higher level feature map generators)\n",
        "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
        "            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
        "\n",
        "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
        "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
        "                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        return locs, classes_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXky2-v5sZom"
      },
      "source": [
        "Nous envoyons d'abord l'entrée à VGGBase, qui renvoie deux vecteurs de caractéristiques de dimensions (N, 512, 38, 38) et (N, 1024, 19, 19). La deuxième sortie sera l'entrée pour AuxiliaryConvolutions, qui renvoie plus de cartes de caractéristiques de dimensions (N, 512, 10, 10), (N, 256, 5, 5), (N, 256, 3, 3) et (N , 256, 1, 1). Enfin, la première sortie de VGGBase et ces quatre featuremaps sont envoyées à PredictionConvolutions, qui renvoie 8 732 boîtes d'ancrage comme nous en avons discuté précédemment.\n",
        "\n",
        "L'autre aspect clé de la classe SSD300 est la méthode create_prior_boxes. Pour chaque carte de caractéristiques, trois éléments lui sont associés : la taille de la grille, l'échelle de réduction de la cellule de la grille (il s'agit de la boîte d'ancrage de base pour cette carte de caractéristiques) et les proportions pour toutes les ancres d'une cellule. À l'aide de ces trois configurations, le code utilise une triple boucle for et crée une liste de (cx, cy, w, h) pour les 8 732 ancres.\n",
        "\n",
        "Enfin, la méthode detect_objects prend des tenseurs de classification et des valeurs de régression (des boîtes d'ancrage prédites) et les convertit en coordonnées réelles de la boîte englobante\n",
        "\n",
        "# MultiBoxLoss\n",
        "\n",
        "En tant qu'humains, nous ne sommes préoccupés que par une poignée de cadres de délimitation. Mais pour le fonctionnement de SSD, nous devons comparer 8 732 cadres de délimitation de plusieurs cartes de caractéristiques et prédire si un cadre d'ancrage contient ou non des informations précieuses.\n",
        "\n",
        " Nous assignons cette tâche de calcul de perte à MultiBoxLoss. L'entrée pour la méthode directe est les prédictions de la boîte d'ancrage du modèle et les boîtes englobantes de la vérité. la boîte englobante.\n",
        " \n",
        "Si l'IoU est suffisamment élevé, cette boîte d'ancrage particulière aura des coordonnées de régression non nulles et associera un objet comme vérité terrain pour la classification. Naturellement, la plupart des boîtes d'ancrage calculées auront leur classe associée en arrière-plan car leur IoU avec la boîte englobante réelle sera minuscule ou, dans de nombreux cas, nulle.\n",
        "\n",
        "Une fois les vérités converties en ces 8 732 tenseurs de régression et de classification des boîtes d'ancrage, il est facile de les comparer aux prédictions du modèle puisque les formes sont désormais les mêmes. eux jusqu'à être retourné comme la perte finale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCcXO4BHt4Xa"
      },
      "source": [
        "## Training SSD on a custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwJkTOaBt8OR",
        "outputId": "466ea908-17bb-4d55-e6e3-ffc049422a49"
      },
      "source": [
        "import os\n",
        "if not os.path.exists('open-images-bus-trucks'):\n",
        "    !pip install -q torch_snippets\n",
        "    !wget --quiet https://www.dropbox.com/s/agmzwk95v96ihic/open-images-bus-trucks.tar.xz\n",
        "    !tar -xf open-images-bus-trucks.tar.xz\n",
        "    !rm open-images-bus-trucks.tar.xz\n",
        "    !git clone https://github.com/sizhky/ssd-utils/\n",
        "%cd ssd-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 30 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 40 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 27.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 44.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.6 MB/s \n",
            "\u001b[?25hCloning into 'ssd-utils'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n",
            "/content/ssd-utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meQP85gUuHoW"
      },
      "source": [
        "\n",
        "from torch_snippets import *\n",
        "DATA_ROOT = '../open-images-bus-trucks/'\n",
        "IMAGE_ROOT = f'{DATA_ROOT}/images'\n",
        "DF_RAW = df = pd.read_csv(f'{DATA_ROOT}/df.csv')\n",
        "\n",
        "df = df[df['ImageID'].isin(df['ImageID'].unique().tolist())]\n",
        "\n",
        "label2target = {l:t+1 for t,l in enumerate(DF_RAW['LabelName'].unique())}\n",
        "label2target['background'] = 0\n",
        "target2label = {t:l for l,t in label2target.items()}\n",
        "background_class = label2target['background']\n",
        "num_classes = len(label2target)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiXYCIyuuPHH"
      },
      "source": [
        "import collections, os, torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "denormalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
        "    std=[1/0.229, 1/0.224, 1/0.255]\n",
        ")\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = torch.tensor(img).permute(2,0,1)\n",
        "    img = normalize(img)\n",
        "    return img.to(device).float()\n",
        "    \n",
        "class OpenDataset(torch.utils.data.Dataset):\n",
        "    w, h = 300, 300\n",
        "    def __init__(self, df, image_dir=IMAGE_ROOT):\n",
        "        self.image_dir = image_dir\n",
        "        self.files = glob.glob(self.image_dir+'/*')\n",
        "        self.df = df\n",
        "        self.image_infos = df.ImageID.unique()\n",
        "        logger.info(f'{len(self)} items loaded')\n",
        "        \n",
        "    def __getitem__(self, ix):\n",
        "        # load images and masks\n",
        "        image_id = self.image_infos[ix]\n",
        "        img_path = find(image_id, self.files)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n",
        "        data = df[df['ImageID'] == image_id]\n",
        "        labels = data['LabelName'].values.tolist()\n",
        "        data = data[['XMin','YMin','XMax','YMax']].values\n",
        "        data[:,[0,2]] *= self.w\n",
        "        data[:,[1,3]] *= self.h\n",
        "        boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n",
        "        return img, boxes, labels\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        images, boxes, labels = [], [], []\n",
        "        for item in batch:\n",
        "            img, image_boxes, image_labels = item\n",
        "            img = preprocess_image(img)[None]\n",
        "            images.append(img)\n",
        "            boxes.append(torch.tensor(image_boxes).float().to(device)/300.)\n",
        "            labels.append(torch.tensor([label2target[c] for c in image_labels]).long().to(device))\n",
        "        images = torch.cat(images).to(device)\n",
        "        return images, boxes, labels\n",
        "    def __len__(self):\n",
        "        return len(self.image_infos)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I492x13eusPo",
        "outputId": "6d8378f4-733e-4728-b095-7e6b8c259ef7"
      },
      "source": [
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "trn_ids, val_ids = train_test_split(df.ImageID.unique(), test_size=0.1, random_state=99)\n",
        "trn_df, val_df = df[df['ImageID'].isin(trn_ids)], df[df['ImageID'].isin(val_ids)]\n",
        "len(trn_df), len(val_df)\n",
        "\n",
        "train_ds = OpenDataset(trn_df)\n",
        "test_ds = OpenDataset(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-15 13:52:29.741 | INFO     | __main__:__init__:27 - 13702 items loaded\n",
            "2021-09-15 13:52:29.789 | INFO     | __main__:__init__:27 - 1523 items loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFyfbsxZuyRI"
      },
      "source": [
        "def train_batch(inputs, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    N = len(train_loader)\n",
        "    images, boxes, labels = inputs\n",
        "    _regr, _clss = model(images)\n",
        "    loss = criterion(_regr, _clss, boxes, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "    \n",
        "@torch.no_grad()\n",
        "def validate_batch(inputs, model, criterion):\n",
        "    model.eval()\n",
        "    images, boxes, labels = inputs\n",
        "    _regr, _clss = model(images)\n",
        "    loss = criterion(_regr, _clss, boxes, labels)\n",
        "    return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kYp2TmSu1Aw"
      },
      "source": [
        "from model import SSD300, MultiBoxLoss\n",
        "from detect import *"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "4734bc67a20a46a9898debd187a2d7bf",
            "c238c56a37ae40fda1f66efc4deeb3ec",
            "5a24825061d5450891e976326117d5f9",
            "8ae62b4ea0f44304a059c4acce1e991d",
            "8a5e63bd511247bb83f3c03a2937c65e",
            "6279e455b6c84080bcbe49aef138887b",
            "2b7240c165e14bb494fe1c11369ec73c",
            "a00f534f65724cddbbedbc5bae5f5b7a",
            "2ddd9303c076452b95cff547c9ba204a",
            "c46bb09d2a244f20842419248c07c6d0",
            "21cf43f5f31d43199d6873b9e3fd2f04"
          ]
        },
        "id": "FgHaOdLgu3lW",
        "outputId": "3d09892f-8cc9-4419-d17f-fc3fc1039dcd"
      },
      "source": [
        "\n",
        "n_epochs = 3\n",
        "\n",
        "model = SSD300(num_classes, device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy, device=device)\n",
        "\n",
        "log = Report(n_epochs=n_epochs)\n",
        "logs_to_print = 5"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4734bc67a20a46a9898debd187a2d7bf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded base model.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rFuOl3Cu7E3",
        "outputId": "cb0ae6a1-db83-4421-9b65-b616762e3c1b"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    _n = len(train_loader)\n",
        "    for ix, inputs in enumerate(train_loader):\n",
        "        loss = train_batch(inputs, model, criterion, optimizer)\n",
        "        pos = (epoch + (ix+1)/_n)\n",
        "        log.record(pos, trn_loss=loss.item(), end='\\r')\n",
        "\n",
        "    _n = len(test_loader)\n",
        "    for ix,inputs in enumerate(test_loader):\n",
        "        loss = validate_batch(inputs, model, criterion)\n",
        "        pos = (epoch + (ix+1)/_n)\n",
        "        log.record(pos, val_loss=loss.item(), end='\\r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0.030\ttrn_loss: 4.434\t(1262.23s - 124654.58s remaining)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JLAyDaNu97p"
      },
      "source": [
        "image_paths = Glob(f'{DATA_ROOT}/images/*')\n",
        "image_id = choose(test_ds.image_infos)\n",
        "img_path = find(image_id, test_ds.files)\n",
        "original_image = Image.open(img_path, mode='r')\n",
        "original_image = original_image.convert('RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsOcXJAuvAcW"
      },
      "source": [
        "image_paths = Glob(f'{DATA_ROOT}/images/*')\n",
        "for _ in range(3):\n",
        "    image_id = choose(test_ds.image_infos)\n",
        "    img_path = find(image_id, test_ds.files)\n",
        "    original_image = Image.open(img_path, mode='r')\n",
        "    bbs, labels, scores = detect(original_image, model, min_score=0.9, max_overlap=0.5,top_k=200, device=device)\n",
        "    labels = [target2label[c.item()] for c in labels]\n",
        "    label_with_conf = [f'{l} @ {s:.2f}' for l,s in zip(labels,scores)]\n",
        "    print(bbs, label_with_conf)\n",
        "    show(original_image, bbs=bbs, texts=label_with_conf, text_sz=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
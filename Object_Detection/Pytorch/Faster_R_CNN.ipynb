{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_R-CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "abd0f19ace104a5f9ea566442c280f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f156cb2696b41dbb5f6d010aa55c8fc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ffdf807149c41f8b73fbfc150c075e8",
              "IPY_MODEL_8ebae83e77fd4be6badb8b6ee9601431",
              "IPY_MODEL_a3908b8422394fe1ac0ef68865d7f7d8"
            ]
          }
        },
        "0f156cb2696b41dbb5f6d010aa55c8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ffdf807149c41f8b73fbfc150c075e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_76ba80ed18444975b6be1d61a14ccb32",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf232ba060fe4f12a2a53fe782d8980d"
          }
        },
        "8ebae83e77fd4be6badb8b6ee9601431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ca07a2e33af74e2cbe9c0c04acba9142",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3da5eecb1c8422990c1153f0e18adf2"
          }
        },
        "a3908b8422394fe1ac0ef68865d7f7d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1655d4130a804f678ae8e40cc8fb9018",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:01&lt;00:00, 112MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49f7bcbd28a04eeb81c36c37a98fc8a4"
          }
        },
        "76ba80ed18444975b6be1d61a14ccb32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf232ba060fe4f12a2a53fe782d8980d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca07a2e33af74e2cbe9c0c04acba9142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a3da5eecb1c8422990c1153f0e18adf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1655d4130a804f678ae8e40cc8fb9018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49f7bcbd28a04eeb81c36c37a98fc8a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYO0a4Gi376o"
      },
      "source": [
        "# Training Faster R-CNN on a customdataset\n",
        "\n",
        "Dans le code suivant, nous allons entraîner l'algorithme Faster R-CNN pour détecter les cadres de délimitation autour des objets présents dans les images. Pour cela, nous allons travailler sur le même exercice de détection camion versus bus que nous avons travaillé dans l'article précédent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "bs1gEz2A4NkR",
        "outputId": "b9729d69-70b8-4d68-d168-4e4f221c97d4"
      },
      "source": [
        "# Download the dataset\n",
        "import os\n",
        "if not os.path.exists('images'):\n",
        "    !pip install -qU torch_snippets\n",
        "    from google.colab import files\n",
        "    files.upload() # upload kaggle.json\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !mv kaggle.json ~/.kaggle/\n",
        "    !ls ~/.kaggle\n",
        "    !chmod 600 /root/.kaggle/kaggle.json\n",
        "    !kaggle datasets download -d sixhky/open-images-bus-trucks/\n",
        "    !unzip -qq open-images-bus-trucks.zip\n",
        "    !rm open-images-bus-trucks.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 20 kB 33.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 30 kB 36.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 40 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 27.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 38.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.0 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-017219a5-0adb-4410-ac83-bf331b15431f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-017219a5-0adb-4410-ac83-bf331b15431f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "kaggle.json\n",
            "Downloading open-images-bus-trucks.zip to /content\n",
            " 96% 353M/367M [00:08<00:00, 30.9MB/s]\n",
            "100% 367M/367M [00:08<00:00, 45.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynQOY8J24QOz",
        "outputId": "03149860-8a64-4115-8af7-b8eb55e01b2d"
      },
      "source": [
        "# Read the DataFrame containing metadata of information about images and their bounding box, and classes\n",
        "from torch_snippets import *\n",
        "from PIL import Image\n",
        "IMAGE_ROOT = 'images/images'\n",
        "DF_RAW = df = pd.read_csv('df.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-15 11:49:31.377 | WARNING  | torch_snippets:<module>:13 - sklearn is not found. Skipping relevant imports from submodule `sklegos`\n",
            "Exception: No module named 'sklego'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA86cB6a4YDk"
      },
      "source": [
        "#Define the indices corresponding to labels and targets\n",
        "\n",
        "label2target = {l:t+1 for t,l in enumerate(DF_RAW['LabelName'].unique())}\n",
        "label2target['background'] = 0\n",
        "target2label = {t:l for l,t in label2target.items()}\n",
        "background_class = label2target['background']\n",
        "num_classes = len(label2target)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wOxxeie4abc"
      },
      "source": [
        "\n",
        "#Define the function to pre-process an image\n",
        "def preprocess_image(img):\n",
        "    img = torch.tensor(img).permute(2,0,1)\n",
        "    return img.to(device).float()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_X2UGRf4dB5"
      },
      "source": [
        "#Define the dataset class\n",
        "import glob\n",
        "class OpenDataset(torch.utils.data.Dataset):\n",
        "    w, h = 224, 224\n",
        "    def __init__(self, df, image_dir=IMAGE_ROOT):\n",
        "        self.image_dir = image_dir\n",
        "        self.files = glob.glob(self.image_dir+'/*')\n",
        "        self.df = df\n",
        "        self.image_infos = df.ImageID.unique()\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        # load images and masks\n",
        "        image_id = self.image_infos[ix]\n",
        "        img_path = find(image_id, self.files)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n",
        "        data = df[df['ImageID'] == image_id]\n",
        "        labels = data['LabelName'].values.tolist()\n",
        "        data = data[['XMin','YMin','XMax','YMax']].values\n",
        "        data[:,[0,2]] *= self.w\n",
        "        data[:,[1,3]] *= self.h\n",
        "        boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n",
        "        # torch FRCNN expects ground truths as a dictionary of tensors\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.Tensor(boxes).float()\n",
        "        target[\"labels\"] = torch.Tensor([label2target[i] for i in labels]).long()\n",
        "        img = preprocess_image(img)\n",
        "        return img, target\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        return tuple(zip(*batch)) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_infos)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opSkPv864g6C"
      },
      "source": [
        "\n",
        "# Create the training and validation dataloaders and datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "trn_ids, val_ids = train_test_split(df.ImageID.unique(), test_size=0.1, random_state=99)\n",
        "trn_df, val_df = df[df['ImageID'].isin(trn_ids)], df[df['ImageID'].isin(val_ids)]\n",
        "len(trn_df), len(val_df)\n",
        "\n",
        "train_ds = OpenDataset(trn_df)\n",
        "test_ds = OpenDataset(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MrQOoQh4k05"
      },
      "source": [
        "#Define the model\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def get_model():\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKWTbQn7Yh2"
      },
      "source": [
        "Le modèle contient les sous-modules clés suivants\n",
        "\n",
        "* **GeneralizedRCNNTransform** : est un redimensionnement simple suivi d'une transformation de normalisation \n",
        "\n",
        "* **BackboneWithFPN**: est un réseau de neurones qui transforme l'entrée en carte de caractéristiques\n",
        "\n",
        "* **RegionProposalNetwork**: génère les boîtes d'ancrage pour la carte de caractéristiques précédente et prédit des cartes de caractéristiques individuelles pour les tâches de classification et de régression\n",
        "\n",
        "* **RoIHeads** : prend les cartes précédentes, les aligne à l'aide du pool de ROI, les traite et renvoie les probabilités de classification pour chaque proposition et les décalages correspondants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_PZG4iZ48_c"
      },
      "source": [
        "# Defining training and validation functions for a single batch\n",
        "def train_batch(inputs, model, optimizer):\n",
        "    model.train()\n",
        "    input, targets = inputs\n",
        "    input = list(image.to(device) for image in input)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(input, targets)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, losses\n",
        "\n",
        "@torch.no_grad() # this will disable gradient computation in the function below\n",
        "def validate_batch(inputs, model):\n",
        "    model.train() # to obtain the losses, model needs to be in train mode only. # #Note that here we are not defining the model's forward method \n",
        "#and hence need to work per the way the model class is defined\n",
        "    input, targets = inputs\n",
        "    input = list(image.to(device) for image in input)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(input, targets)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    return loss, losses"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "abd0f19ace104a5f9ea566442c280f6c",
            "0f156cb2696b41dbb5f6d010aa55c8fc",
            "9ffdf807149c41f8b73fbfc150c075e8",
            "8ebae83e77fd4be6badb8b6ee9601431",
            "a3908b8422394fe1ac0ef68865d7f7d8",
            "76ba80ed18444975b6be1d61a14ccb32",
            "bf232ba060fe4f12a2a53fe782d8980d",
            "ca07a2e33af74e2cbe9c0c04acba9142",
            "a3da5eecb1c8422990c1153f0e18adf2",
            "1655d4130a804f678ae8e40cc8fb9018",
            "49f7bcbd28a04eeb81c36c37a98fc8a4"
          ]
        },
        "id": "xUvigZJS5B64",
        "outputId": "1ab0256a-043a-4f6d-fbd7-21eb2e22ab8e"
      },
      "source": [
        "model = get_model().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "n_epochs = 5\n",
        "log = Report(n_epochs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abd0f19ace104a5f9ea566442c280f6c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmNCa-TP5Ecu",
        "outputId": "92e7267f-2420-48d1-94d7-16f3843dc967"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    _n = len(train_loader)\n",
        "    for ix, inputs in enumerate(train_loader):\n",
        "        loss, losses = train_batch(inputs, model, optimizer)\n",
        "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n",
        "            [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n",
        "        pos = (epoch + (ix+1)/_n)\n",
        "        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss.item(), \n",
        "                   trn_regr_loss=regr_loss.item(), trn_objectness_loss=loss_objectness.item(),\n",
        "                   trn_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n",
        "\n",
        "    _n = len(test_loader)\n",
        "    for ix,inputs in enumerate(test_loader):\n",
        "        loss, losses = validate_batch(inputs, model)\n",
        "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n",
        "          [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n",
        "        pos = (epoch + (ix+1)/_n)\n",
        "        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss.item(), \n",
        "                  val_regr_loss=regr_loss.item(), val_objectness_loss=loss_objectness.item(),\n",
        "                  val_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n",
        "    if (epoch+1)%(n_epochs//5)==0: log.report_avgs(epoch+1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1.000\ttrn_loss: 0.187\ttrn_loc_loss: 0.077\ttrn_regr_loss: 0.084\ttrn_objectness_loss: 0.017\ttrn_rpn_box_reg_loss: 0.008\tval_loss: 0.172\tval_loc_loss: 0.069\tval_regr_loss: 0.080\tval_objectness_loss: 0.014\tval_rpn_box_reg_loss: 0.009\t(7211.31s - 28845.25s remaining)\n",
            "EPOCH: 1.213\ttrn_loss: 0.112\ttrn_loc_loss: 0.044\ttrn_regr_loss: 0.052\ttrn_objectness_loss: 0.011\ttrn_rpn_box_reg_loss: 0.005\t(8670.07s - 27055.38s remaining)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hqTXzF75HmX"
      },
      "source": [
        "log.plot_epochs(['trn_loss','val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSj8uL8g5KI1"
      },
      "source": [
        "from torchvision.ops import nms\n",
        "def decode_output(output):\n",
        "    'convert tensors to numpy arrays'\n",
        "    bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n",
        "    labels = np.array([target2label[i] for i in output['labels'].cpu().detach().numpy()])\n",
        "    confs = output['scores'].cpu().detach().numpy()\n",
        "    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n",
        "    bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n",
        "\n",
        "    if len(ixs) == 1:\n",
        "        bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n",
        "    return bbs.tolist(), confs.tolist(), labels.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRZ6AE9E5NHS"
      },
      "source": [
        "model.eval()\n",
        "for ix, (images, targets) in enumerate(test_loader):\n",
        "    if ix==3: break\n",
        "    images = [im for im in images]\n",
        "    outputs = model(images)\n",
        "    for ix, output in enumerate(outputs):\n",
        "        bbs, confs, labels = decode_output(output)\n",
        "        info = [f'{l}@{c:.2f}' for l,c in zip(labels, confs)]\n",
        "        show(images[ix].cpu().permute(1,2,0), bbs=bbs, texts=labels, sz=5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}